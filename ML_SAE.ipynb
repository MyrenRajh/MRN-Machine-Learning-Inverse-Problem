{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import torch as to\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import torch.utils.data as to_data\n",
    "from torch.utils.tensorboard import SummaryWriter as sumwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hardware for ML training (GPU default)\n",
    "device = \"cuda\" if to.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly generate list of strings for frequency numbers and ratios\n",
    "def freq_name(no_freq, include_freq=True, include_ratio=True):\n",
    "    \"\"\"\n",
    "    Creates an ordered list of string from inputted parameters:\n",
    "\n",
    "    no_freq = (int) number of desired frequencies\n",
    "    include_freq = (bool) include the individual frequencies or not (default True)\n",
    "    include_ratio = (bool) include the non-trivial ratios between frequencies or not (default True)\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    if include_freq:\n",
    "        for i in range(no_freq):\n",
    "            names.append('f'+str(i+1))\n",
    "    if include_ratio:\n",
    "        for i in range(no_freq):\n",
    "            for j in range(i):\n",
    "                names.append('f'+str(i+1)+'/f'+str(j+1))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pytorch dataset class for data batching during training\n",
    "class SAE_data(to_data.Dataset):\n",
    "    def __init__(self, scaled_dataframe, X_names, Y_names):\n",
    "        self.len = len(scaled_dataframe)\n",
    "        self.X = to.from_numpy(scaled_dataframe[X_names].to_numpy().astype('float32')).to(device)\n",
    "        self.Y = to.from_numpy(scaled_dataframe[Y_names].to_numpy().astype('float32')).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        X_idx = self.X[idx,:]\n",
    "        Y_idx = self.Y[idx,:]\n",
    "        return X_idx, Y_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(activ_name):\n",
    "    if activ_name=='relu':\n",
    "        return to.nn.ReLU()\n",
    "    elif activ_name=='lrelu':\n",
    "        return to.nn.LeakyReLU()\n",
    "    elif activ_name=='prelu':\n",
    "        return to.nn.PReLU()\n",
    "    elif activ_name=='relu6':\n",
    "        return to.nn.ReLU6()\n",
    "    elif activ_name=='sigmoid':\n",
    "        return to.nn.Sigmoid()\n",
    "    elif activ_name=='tanh':\n",
    "        return to.nn.Tanh()\n",
    "    elif activ_name=='silu':\n",
    "        return to.nn.SiLU()\n",
    "    elif activ_name=='selu':\n",
    "        return to.nn.SELU()\n",
    "    elif activ_name=='celu':\n",
    "        return to.nn.CELU()\n",
    "    elif activ_name=='gelu':\n",
    "        return to.nn.GELU()\n",
    "    else:\n",
    "        return to.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE_Network(to.nn.Module):\n",
    "    def __init__(self, num_X, num_Y, he_nodes, hd_nodes, hactiv_type):\n",
    "        super(SAE_Network, self).__init__()\n",
    "\n",
    "        self.encoder = []\n",
    "        self.encoder.append(to.nn.Linear(num_X, he_nodes[0]))\n",
    "        self.encoder.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(he_nodes)-1):\n",
    "            self.encoder.append(to.nn.Linear(he_nodes[i], he_nodes[i+1]))\n",
    "            self.encoder.append(activation(hactiv_type))\n",
    "\n",
    "        self.encoder.append(to.nn.Linear(he_nodes[-1], num_Y))\n",
    "\n",
    "        self.encoder = to.nn.Sequential(*self.encoder).to(device)\n",
    "        for i in self.encoder[::2]:\n",
    "            to.nn.init.xavier_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "        self.decoder = []\n",
    "        self.decoder.append(to.nn.Linear(num_Y, hd_nodes[0]))\n",
    "        self.decoder.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(hd_nodes)-1):\n",
    "            self.decoder.append(to.nn.Linear(hd_nodes[i], hd_nodes[i+1]))\n",
    "            self.decoder.append(activation(hactiv_type))\n",
    "\n",
    "        self.decoder.append(to.nn.Linear(hd_nodes[-1], num_X))\n",
    "        \n",
    "        self.decoder = to.nn.Sequential(*self.decoder).to(device)\n",
    "        for i in self.decoder[::2]:\n",
    "            to.nn.init.xavier_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.encoder(X)\n",
    "        Xpred = self.decoder(Y)\n",
    "        return Y, Xpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network, num_freq,\n",
    "    train_dataloader,\n",
    "    loss_function, optimizer,\n",
    "    tb_writer, epoch_ind\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "    loss_listY = []\n",
    "    loss_listX = []\n",
    "\n",
    "    MAPE_listY = []\n",
    "    MAPE_listX = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        X, Y = data\n",
    "\n",
    "        if epoch_ind==0 and i==0:\n",
    "            tb_writer.add_graph(network, X, verbose=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictY, predictX = network(X)\n",
    "\n",
    "        lossY = loss_function(predictY, Y)\n",
    "        lossX = loss_function(predictX, X)\n",
    "        loss = 3*lossY + lossX\n",
    "\n",
    "        loss_listY.append(lossY.item())\n",
    "        loss_listX.append(lossX.item())\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        X[:,0:num_freq] = to.exp(X[:,0:num_freq])\n",
    "        predictX[:,0:num_freq] = to.exp(predictX[:,0:num_freq])\n",
    " \n",
    "        MAPEY = to.mean(to.abs((Y - predictY) / Y)*100)\n",
    "        MAPEX = to.mean(to.abs((X - predictX) / X)*100)\n",
    "\n",
    "        MAPE_listY.append(MAPEY.item())\n",
    "        MAPE_listX.append(MAPEX.item())\n",
    "    \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "    mean_lossY = to.mean(to.tensor(loss_listY, device=device)).item()\n",
    "    mean_lossX = to.mean(to.tensor(loss_listX, device=device)).item()\n",
    "\n",
    "    mean_MAPEY = to.mean(to.tensor(MAPE_listY, device=device)).item()\n",
    "    mean_MAPEX = to.mean(to.tensor(MAPE_listX, device=device)).item()\n",
    "\n",
    "    return mean_loss, mean_lossY, mean_lossX, mean_MAPEY, mean_MAPEX\n",
    "\n",
    "def valid_epoch(\n",
    "    network, num_freq,\n",
    "    valid_dataloader,\n",
    "    loss_function\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "    loss_listY = []\n",
    "    loss_listX = []\n",
    "\n",
    "    MAPE_listY = []\n",
    "    MAPE_listX = []\n",
    "\n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        X, Y = data\n",
    "        predictY, predictX = network(X)\n",
    "\n",
    "        lossY = loss_function(predictY, Y)\n",
    "        lossX = loss_function(predictX, X)\n",
    "        loss = lossY + lossX\n",
    "\n",
    "        loss_listY.append(lossY.item())\n",
    "        loss_listX.append(lossX.item())\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        X[:,0:num_freq] = to.exp(X[:,0:num_freq])\n",
    "        predictX[:,0:num_freq] = to.exp(predictX[:,0:num_freq])\n",
    " \n",
    "        MAPEY = to.mean(to.abs((Y - predictY) / Y)*100)\n",
    "        MAPEX = to.mean(to.abs((X - predictX) / X)*100)\n",
    "\n",
    "        MAPE_listY.append(MAPEY.item())\n",
    "        MAPE_listX.append(MAPEX.item())\n",
    "    \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "    mean_lossY = to.mean(to.tensor(loss_listY, device=device)).item()\n",
    "    mean_lossX = to.mean(to.tensor(loss_listX, device=device)).item()\n",
    "\n",
    "    mean_MAPEY = to.mean(to.tensor(MAPE_listY, device=device)).item()\n",
    "    mean_MAPEX = to.mean(to.tensor(MAPE_listX, device=device)).item()\n",
    "\n",
    "    return mean_loss, mean_lossY, mean_lossX, mean_MAPEY, mean_MAPEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AEI(\n",
    "    network, num_freq,\n",
    "    train_dataloader, valid_dataloader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate\n",
    "    ):\n",
    "\n",
    "    if optimizer_type=='adam':\n",
    "        optimizer = to.optim.Adam(network.parameters(), lr=learn_rate)\n",
    "    else:\n",
    "        optimizer = to.optim.SGD(network.parameters(), lr=learn_rate)\n",
    "    \n",
    "    tb_writer = sumwriter()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        network.train(True)\n",
    "        mloss, mlossY, mlossX, mMAPEY, mMAPEX = train_epoch(network, num_freq, train_dataloader, loss_function, optimizer, tb_writer, i)\n",
    "\n",
    "        network.eval()\n",
    "        with to.no_grad():\n",
    "            vmloss, vmlossY, vmlossX, vmMAPEY, vmMAPEX = valid_epoch(network, num_freq, valid_dataloader, loss_function)\n",
    "        \n",
    "\n",
    "        print('-'*50)\n",
    "        print('Epoch {} / {}'.format(i+1,epochs))\n",
    "        print('-'*15)\n",
    "        print('Average Train Loss : {}'.format(mloss))\n",
    "        print('Average Validation Loss : {}'.format(vmloss))\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch Mean Loss\",\n",
    "                            {\n",
    "                                'Train' : mloss,\n",
    "                                'Validation' : vmloss\n",
    "                            }, i+1)\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch MAPE - Latent Space\",\n",
    "                            {\n",
    "                                'Train' : mMAPEY,\n",
    "                                'Validation' : vmMAPEY\n",
    "                            }, i+1)\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch MAPE - Reconstruction\",\n",
    "                            {\n",
    "                                'Train' : mMAPEX,\n",
    "                                'Validation' : vmMAPEX\n",
    "                            }, i+1)\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch Mean Loss - Latent Space\",\n",
    "                            {\n",
    "                                'Train' : mlossY,\n",
    "                                'Validation' : vmlossY\n",
    "                            }, i+1)\n",
    "    \n",
    "        tb_writer.add_scalars(\"Batch Mean Loss - Reconstruction\",\n",
    "                            {\n",
    "                                'Train' : mlossX,\n",
    "                                'Validation' : vmlossX\n",
    "                            }, i+1)\n",
    "                            \n",
    "    tb_writer.flush()\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "data = pd.read_csv('TrainValid_Data.csv')\n",
    "data['psi'] =  12*data['rho']/ (data['E']*data['t']**2)\n",
    "num_freq = 10\n",
    "\n",
    "features = freq_name(num_freq,1,0)\n",
    "latent = ['psi', 'nu','a', 'b']\n",
    "\n",
    "train_split = int(0.8*len(data))\n",
    "valid_split = len(data)- train_split\n",
    "\n",
    "scaled_data = data[features+latent].copy()\n",
    "\n",
    "scaled_data['psi'] = np.log(scaled_data['psi'])\n",
    "scaled_data[freq_name(num_freq,1,0)] = np.log(scaled_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scaled_data = SAE_data(scaled_data, features, latent)\n",
    "train_set, valid_set = to_data.random_split(scaled_data, [train_split, valid_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "# Parameters\n",
    "num_features = len(features)\n",
    "num_latent = len(latent)\n",
    "he_nodes = [20,25,30,35,40,35,30,25,20]\n",
    "hd_nodes = [20,25,30,35,40,35,30,25,20]\n",
    "hactiv = 'prelu'\n",
    "\n",
    "batch_size_train = 200\n",
    "batch_size_valid = 2000\n",
    "\n",
    "epochs = 200\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# Optim Selections\n",
    "loss_function = to.nn.SmoothL1Loss()\n",
    "optimizer_type = 'adam'\n",
    "\n",
    "# Data loaders\n",
    "train_loader = to.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True)\n",
    "valid_loader = to.utils.data.DataLoader(valid_set, batch_size=batch_size_valid, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = SAE_Network(num_features, num_latent, he_nodes, hd_nodes, hactiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_AEI(\n",
    "    model, num_freq,\n",
    "    train_loader, valid_loader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate)\n",
    "\n",
    "to.save(model.state_dict(), 'SAE_model.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(to.load('SAE_model.state'))\n",
    "\n",
    "test_data = pd.read_csv('Test_Data.csv')\n",
    "test_data['psi'] =  12*test_data['rho']/ (test_data['E']*test_data['t']**2)\n",
    "\n",
    "train_split = int(0.8*len(data))\n",
    "valid_split = len(data)- train_split\n",
    "\n",
    "scaled_test_data = test_data[features+latent].copy()\n",
    "\n",
    "scaled_test_data['psi'] = np.log(scaled_test_data['psi'])\n",
    "scaled_test_data[freq_name(num_freq,1,0)] = np.log(scaled_test_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scaled_test_data = SAE_data(scaled_test_data, features, latent)\n",
    "test_loader = to.utils.data.DataLoader(scaled_test_data, batch_size=len(scaled_test_data), shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with to.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        X, Y = data\n",
    "        predictY = model.encoder(X)\n",
    "\n",
    "        Y[:,0] = to.exp(Y[:,0])\n",
    "        abs_perc_error = to.abs((Y - predictY)/Y)*100\n",
    "        MAPE_per_dim = to.mean(abs_perc_error, 0)\n",
    "\n",
    "        print('Absolute Percentage Errors: ')\n",
    "        print('-'*20)\n",
    "        print('E : {:0.2f} %'.format(MAPE_per_dim[0].item()))\n",
    "        print('nu : {:0.2f} %'.format(MAPE_per_dim[1].item()))\n",
    "        print('rho : {:0.2f} %'.format(MAPE_per_dim[2].item()))\n",
    "        print('a : {:0.2f} %'.format(MAPE_per_dim[3].item()))\n",
    "        # print('b : {:0.2f} %'.format(MAPE_per_dim[4].item()))\n",
    "        # print('t : {:0.2f} %'.format(MAPE_per_dim[5].item()))\n",
    "\n",
    "        # np.savetxt('SAE_MAPE_test.txt', MAPE_per_dim.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2426e189533ac41f5aad4fbc5dc1b573401ba6ae3fed99d7f2363912ea400c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
