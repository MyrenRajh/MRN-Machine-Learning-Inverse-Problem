{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import torch as to\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import torch.utils.data as to_data\n",
    "from torch.utils.tensorboard import SummaryWriter as sumwriter\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hardware for ML training (GPU default)\n",
    "device = \"cuda\" if to.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly generate list of strings for frequency numbers and ratios\n",
    "def freq_name(no_freq, include_freq=True, include_ratio=True):\n",
    "    \"\"\"\n",
    "    Creates an ordered list of string from inputted parameters:\n",
    "\n",
    "    no_freq = (int) number of desired frequencies\n",
    "    include_freq = (bool) include the individual frequencies or not (default True)\n",
    "    include_ratio = (bool) include the non-trivial ratios between frequencies or not (default True)\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    if include_freq:\n",
    "        for i in range(no_freq):\n",
    "            names.append('f'+str(i+1))\n",
    "    if include_ratio:\n",
    "        for i in range(no_freq):\n",
    "            for j in range(i):\n",
    "                names.append('f'+str(i+1)+'/f'+str(j+1))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(activ_name):\n",
    "    if activ_name=='relu':\n",
    "        return to.nn.ReLU()\n",
    "    elif activ_name=='lrelu':\n",
    "        return to.nn.LeakyReLU()\n",
    "    elif activ_name=='prelu':\n",
    "        return to.nn.PReLU()\n",
    "    elif activ_name=='relu6':\n",
    "        return to.nn.ReLU6()\n",
    "    elif activ_name=='sigmoid':\n",
    "        return to.nn.Sigmoid()\n",
    "    elif activ_name=='tanh':\n",
    "        return to.nn.Tanh()\n",
    "    elif activ_name=='silu':\n",
    "        return to.nn.SiLU()\n",
    "    elif activ_name=='selu':\n",
    "        return to.nn.SELU()\n",
    "    elif activ_name=='celu':\n",
    "        return to.nn.CELU()\n",
    "    elif activ_name=='gelu':\n",
    "        return to.nn.GELU()\n",
    "    else:\n",
    "        return to.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE_data(to_data.Dataset):\n",
    "    def __init__(self, scaled_dataframe, X_names, Y_names):\n",
    "        self.len = len(scaled_dataframe)\n",
    "        self.X = to.from_numpy(scaled_dataframe[X_names].to_numpy().astype('float32')).to(device)\n",
    "        self.Y = to.from_numpy(scaled_dataframe[Y_names].to_numpy().astype('float32')).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        X_idx = self.X[idx,:]\n",
    "        Y_idx = self.Y[idx,:]\n",
    "        return X_idx, Y_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE_Network(to.nn.Module):\n",
    "    def __init__(self, num_X, num_Y, num_Z, he_nodes, hd_nodes, hactiv_type):\n",
    "        super(SAE_Network, self).__init__()\n",
    "\n",
    "        self.esec = []\n",
    "        self.esec.append(to.nn.Linear(num_X+num_Y, he_nodes[0]))\n",
    "        self.esec.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(he_nodes)-1):\n",
    "            self.esec.append(to.nn.Linear(he_nodes[i], he_nodes[i+1]))\n",
    "            self.esec.append(activation(hactiv_type))\n",
    "\n",
    "        self.esec.append(to.nn.Linear(he_nodes[-1], 2*num_Z))\n",
    "\n",
    "        self.esec = to.nn.Sequential(*self.esec).to(device)\n",
    "        for i in self.esec[::2]:\n",
    "            to.nn.init.xavier_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "        self.mu_layer = to.nn.Linear(2*num_Z, num_Z).to(device)\n",
    "        to.nn.init.xavier_uniform_(self.mu_layer.weight)\n",
    "        to.nn.init.zeros_(self.mu_layer.bias)\n",
    "\n",
    "        self.var_layer = to.nn.Linear(2*num_Z, num_Z).to(device)\n",
    "        to.nn.init.xavier_uniform_(self.var_layer.weight)\n",
    "        to.nn.init.zeros_(self.var_layer.bias)\n",
    "\n",
    "\n",
    "        self.dsec = []\n",
    "        self.dsec.append(to.nn.Linear(num_X+num_Z, hd_nodes[0]))\n",
    "        self.dsec.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(hd_nodes)-1):\n",
    "            self.dsec.append(to.nn.Linear(hd_nodes[i], hd_nodes[i+1]))\n",
    "            self.dsec.append(activation(hactiv_type))\n",
    "\n",
    "        self.dsec.append(to.nn.Linear(hd_nodes[-1], num_Y))\n",
    "        \n",
    "        self.dsec = to.nn.Sequential(*self.dsec).to(device)\n",
    "        for i in self.dsec[::2]:\n",
    "            to.nn.init.xavier_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = to.exp(0.5*logvar)\n",
    "        eps = to.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def encode(self, X, Y):\n",
    "        inputs = to.cat([Y, X], 1)\n",
    "        Zp = self.esec(inputs)\n",
    "        Z_mu = self.mu_layer(Zp)\n",
    "        Z_var = self.var_layer(Zp)\n",
    "        return Z_mu, Z_var\n",
    "    \n",
    "    def decode(self, X, Z): # P(x|z, c)\n",
    "        inputs = to.cat([Z, X], 1) # (bs, latent_size+class_size)\n",
    "        Yp = self.dsec(inputs)\n",
    "        return Yp\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        Z_mu, Z_logvar = self.encode(X, Y)\n",
    "        Z = self.reparameterize(Z_mu, Z_logvar)\n",
    "        Yp = self.decode(X, Z)\n",
    "        return Yp, Z_mu, Z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network, num_freq,\n",
    "    train_dataloader,\n",
    "    loss_function, optimizer,\n",
    "    tb_writer, epoch_ind\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        X, Y = data\n",
    "\n",
    "        # if epoch_ind==0 and i==0:\n",
    "        #     tb_writer.add_graph(network, (Y,X), verbose=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        Yp, Z_mu, Z_logvar = network(X, Y)\n",
    "\n",
    "        loss = loss_function(Yp, Y) -0.5 * to.sum(1 + Z_logvar - Z_mu.pow(2) - Z_logvar.exp())\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "def valid_epoch(\n",
    "    network, num_freq,\n",
    "    valid_dataloader,\n",
    "    loss_function\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        X, Y = data\n",
    "        Yp, Z_mu, Z_logvar = network(X, Y)\n",
    "        loss = loss_function(Yp, Y) -0.5 * to.sum(1 + Z_logvar - Z_mu.pow(2) - Z_logvar.exp())\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AEI(\n",
    "    network, num_freq,\n",
    "    train_dataloader, valid_dataloader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate\n",
    "    ):\n",
    "\n",
    "    if optimizer_type=='adam':\n",
    "        optimizer = to.optim.Adam(network.parameters(), lr=learn_rate)\n",
    "    else:\n",
    "        optimizer = to.optim.SGD(network.parameters(), lr=learn_rate)\n",
    "    \n",
    "    tb_writer = sumwriter('Current_ML_Results/Tensorboard')\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        network.train(True)\n",
    "        mloss = train_epoch(network, num_freq, train_dataloader, loss_function, optimizer, tb_writer, i)\n",
    "\n",
    "        network.eval()\n",
    "        with to.no_grad():\n",
    "            vmloss = valid_epoch(network, num_freq, valid_dataloader, loss_function)\n",
    "        \n",
    "\n",
    "        print('-'*50)\n",
    "        print('Epoch {} / {}'.format(i+1,epochs))\n",
    "        print('-'*15)\n",
    "        print('Average Train Loss : {}'.format(mloss))\n",
    "        print('Average Validation Loss : {}'.format(vmloss))\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch Mean Loss\",\n",
    "                            {\n",
    "                                'Train' : mloss,\n",
    "                                'Validation' : vmloss\n",
    "                            }, i+1)\n",
    "                            \n",
    "    tb_writer.flush()\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "data = pd.read_csv('Data Workspace\\SA_TV_Data.csv')\n",
    "num_freq = 6\n",
    "\n",
    "X_names = freq_name(num_freq,1,0)+['E', 'rho', 't']\n",
    "Y_names = ['nu','a', 'b']\n",
    "\n",
    "train_split = int(0.8*len(data))\n",
    "valid_split = len(data)- train_split\n",
    "\n",
    "scaled_data = data[X_names+Y_names].copy()\n",
    "\n",
    "# scaled_data['psi'] = np.log(scaled_data['psi'])\n",
    "scaled_data['E'] = scaled_data['E']/1e11\n",
    "scaled_data['rho'] = scaled_data['rho']/10000\n",
    "scaled_data['t'] = scaled_data['t']*100\n",
    "scaled_data[freq_name(num_freq,1,0)] = np.log(scaled_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scaled_data = SAE_data(scaled_data, X_names, Y_names)\n",
    "train_set, valid_set = to_data.random_split(scaled_data, [train_split, valid_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "# Parameters\n",
    "num_X = len(X_names)\n",
    "num_Y = len(Y_names)\n",
    "num_Z = 10\n",
    "he_nodes = [20,25,30,35,30,25,20]\n",
    "hd_nodes = [20,25,30,35,30,25,20]\n",
    "hactiv = 'silu'\n",
    "\n",
    "batch_size_train = 200\n",
    "batch_size_valid = 2000\n",
    "\n",
    "epochs = 200\n",
    "learn_rate = 1e-4\n",
    "\n",
    "# Optim Selections\n",
    "loss_function = to.nn.SmoothL1Loss()\n",
    "optimizer_type = 'adam'\n",
    "\n",
    "# Data loaders\n",
    "train_loader = to.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True)\n",
    "valid_loader = to.utils.data.DataLoader(valid_set, batch_size=batch_size_valid, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = SAE_Network(num_X, num_Y, num_Z, he_nodes, hd_nodes, hactiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Epoch 72 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.0032142281997948885\n",
      "Average Validation Loss : 0.005223519168794155\n",
      "--------------------------------------------------\n",
      "Epoch 73 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.0031996802426874638\n",
      "Average Validation Loss : 0.004987047053873539\n",
      "--------------------------------------------------\n",
      "Epoch 74 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.003185860114172101\n",
      "Average Validation Loss : 0.004861749708652496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\myren\\My Drive\\University - BEng Mechanical\\4th Year\\MRN 412_422\\Revised Code\\ML_cVAE.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_AEI(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model, num_freq,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_loader, valid_loader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     loss_function, optimizer_type,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     epochs, learn_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m to\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mSAE_model.state\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\myren\\My Drive\\University - BEng Mechanical\\4th Year\\MRN 412_422\\Revised Code\\ML_cVAE.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_AEI\u001b[1;34m(network, num_freq, train_dataloader, valid_dataloader, loss_function, optimizer_type, epochs, learn_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     network\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     mloss \u001b[39m=\u001b[39m train_epoch(network, num_freq, train_dataloader, loss_function, optimizer, tb_writer, i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     network\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mwith\u001b[39;00m to\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32mc:\\Users\\myren\\My Drive\\University - BEng Mechanical\\4th Year\\MRN 412_422\\Revised Code\\ML_cVAE.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(network, num_freq, train_dataloader, loss_function, optimizer, tb_writer, epoch_ind)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     loss_list\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m mean_loss \u001b[39m=\u001b[39m to\u001b[39m.\u001b[39mmean(to\u001b[39m.\u001b[39mtensor(loss_list, device\u001b[39m=\u001b[39mdevice))\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_cVAE.ipynb#X31sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mean_loss\n",
      "File \u001b[1;32mc:\\Users\\myren\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\myren\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\myren\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\myren\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\myren\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:256\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_AEI(\n",
    "    model, num_freq,\n",
    "    train_loader, valid_loader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate)\n",
    "\n",
    "to.save(model.state_dict(), 'SAE_model.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2977, 0.2106, 0.2159]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with to.no_grad():\n",
    "    # freqs = to.tensor([[347.65625, 515.625, 605.46875, 882.8125, 882.8125, 1550.7812, 1550.7812, 1558.5937, 1746.0937, 1953.125]],device=device)\n",
    "    freqs = to.tensor([[371.875, 480, 1029.375, 1049.375, 1798.125, 1856.875,]],device=device)\n",
    "    props = to.tensor([[70e-2, 2700/1e4, 5.90/10]],device=device)\n",
    "    inp = to.cat((to.log(freqs),props),1)\n",
    "    noise = to.randn((1,num_Z), device=device)\n",
    "    prediction = model.decode(inp,noise)\n",
    "    # prediction[:,0] = to.exp(prediction[:,0])\n",
    "    print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2426e189533ac41f5aad4fbc5dc1b573401ba6ae3fed99d7f2363912ea400c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
