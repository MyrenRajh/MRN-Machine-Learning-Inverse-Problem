{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"BCbukVX6ROne"},"outputs":[],"source":["# Importing modules\n","import torch as to\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn as sk\n","import torch.utils.data as to_data\n","from torch.utils.tensorboard import SummaryWriter as sumwriter"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"30CxoqE_ROnh","outputId":"40becbbe-9d9b-4770-b5be-d8bb8788269b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]}],"source":["# Specify hardware for ML training (GPU default)\n","device = \"cuda\" if to.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"jwjIxIH_ROni"},"outputs":[],"source":["# Quickly generate list of strings for frequency numbers and ratios\n","def freq_name(no_freq, include_freq=True, include_ratio=True):\n","    \"\"\"\n","    Creates an ordered list of string from inputted parameters:\n","\n","    no_freq = (int) number of desired frequencies\n","    include_freq = (bool) include the individual frequencies or not (default True)\n","    include_ratio = (bool) include the non-trivial ratios between frequencies or not (default True)\n","    \"\"\"\n","    names = []\n","    if include_freq:\n","        for i in range(no_freq):\n","            names.append('f'+str(i+1))\n","    if include_ratio:\n","        for i in range(no_freq):\n","            for j in range(i):\n","                names.append('f'+str(i+1)+'/f'+str(j+1))\n","    return names"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2Lqwg4uUROnj"},"outputs":[],"source":["# Create Pytorch dataset class for data batching during training\n","class GSAE_data(to_data.Dataset):\n","    def __init__(self, scaled_dataframe, X_names, Y_names):\n","        self.len = len(scaled_dataframe)\n","        self.X = to.from_numpy(scaled_dataframe[X_names].to_numpy().astype('float32')).to(device)\n","        self.Y = to.from_numpy(scaled_dataframe[Y_names].to_numpy().astype('float32')).to(device)\n","\n","    def __len__(self):\n","        return self.len\n","  \n","    def __getitem__(self, idx):\n","        X_idx = self.X[idx,:]\n","        Y_idx = self.Y[idx,:]\n","        return X_idx, Y_idx"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"J5BbAYl5ROnk"},"outputs":[],"source":["def activation(activ_name):\n","    if activ_name=='relu':\n","        return to.nn.ReLU()\n","    elif activ_name=='lrelu':\n","        return to.nn.LeakyReLU()\n","    elif activ_name=='prelu':\n","        return to.nn.PReLU()\n","    elif activ_name=='relu6':\n","        return to.nn.ReLU6()\n","    elif activ_name=='sigmoid':\n","        return to.nn.Sigmoid()\n","    elif activ_name=='tanh':\n","        return to.nn.Tanh()\n","    elif activ_name=='silu':\n","        return to.nn.SiLU()\n","    elif activ_name=='selu':\n","        return to.nn.SELU()\n","    elif activ_name=='celu':\n","        return to.nn.CELU()\n","    elif activ_name=='gelu':\n","        return to.nn.GELU()\n","    else:\n","        return to.nn.ReLU()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"oHbVTxliROnk"},"outputs":[],"source":["class GSAE_Network(to.nn.Module):\n","    def __init__(self, num_X, num_Y, num_Z, he_nodes, hd_nodes, hactiv_type):\n","        super(GSAE_Network, self).__init__()\n","\n","        self.encoder = []\n","        self.encoder.append(to.nn.Linear(num_X, he_nodes[0]))\n","        self.encoder.append(activation(hactiv_type))\n","\n","        for i in range(len(he_nodes)-1):\n","            self.encoder.append(to.nn.Linear(he_nodes[i], he_nodes[i+1]))\n","            self.encoder.append(activation(hactiv_type))\n","\n","        self.encoder.append(to.nn.Linear(he_nodes[-1], num_Y))\n","\n","        self.encoder = to.nn.Sequential(*self.encoder).to(device)\n","        for i in self.encoder[::2]:\n","            to.nn.init.xavier_uniform_(i.weight)\n","            to.nn.init.zeros_(i.bias)\n","\n","        self.decoder = []\n","        self.decoder.append(to.nn.Linear(num_Y, hd_nodes[0]))\n","        self.decoder.append(activation(hactiv_type))\n","\n","        for i in range(len(hd_nodes)-1):\n","            self.decoder.append(to.nn.Linear(hd_nodes[i], hd_nodes[i+1]))\n","            self.decoder.append(activation(hactiv_type))\n","\n","        self.decoder.append(to.nn.Linear(hd_nodes[-1], num_Z))\n","        \n","        self.decoder = to.nn.Sequential(*self.decoder).to(device)\n","        for i in self.decoder[::2]:\n","            to.nn.init.xavier_uniform_(i.weight)\n","            to.nn.init.zeros_(i.bias)\n","\n","    def forward(self, X):\n","        Y = self.encoder(X)\n","        Z = self.decoder(Y)\n","        return Y, Z"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eiTLWwiZROnl"},"outputs":[],"source":["def train_epoch(\n","    network,num_freq_in, num_freq_out,\n","    train_dataloader,\n","    loss_function, optimizer,\n","    tb_writer, epoch_ind\n","    ):\n","\n","    loss_list = []\n","    loss_listY = []\n","    loss_listX = []\n","\n","    MAPE_listY = []\n","    MAPE_listX = []\n","\n","    for i, data in enumerate(train_dataloader):\n","        X, Y = data\n","\n","        if epoch_ind==0 and i==0:\n","            tb_writer.add_graph(network, X[:,0:num_freq_in], verbose=False)\n","\n","        optimizer.zero_grad()\n","        predictY, predictX = network(X[:,0:num_freq_in])\n","\n","        lossY = loss_function(predictY, Y)\n","        lossX = loss_function(predictX, X)\n","        loss = lossY + 2*lossX\n","\n","        loss_listY.append(lossY.item())\n","        loss_listX.append(lossX.item())\n","        loss_list.append(loss.item())\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        X[:,0:num_freq_out] = to.exp(X[:,0:num_freq_out])\n","        predictX[:,0:num_freq_out] = to.exp(predictX[:,0:num_freq_out])\n"," \n","        MAPEY = to.mean(to.abs((Y - predictY) / Y)*100)\n","        MAPEX = to.mean(to.abs((X - predictX) / X)*100)\n","\n","        MAPE_listY.append(MAPEY.item())\n","        MAPE_listX.append(MAPEX.item())\n","    \n","    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n","    mean_lossY = to.mean(to.tensor(loss_listY, device=device)).item()\n","    mean_lossX = to.mean(to.tensor(loss_listX, device=device)).item()\n","\n","    mean_MAPEY = to.mean(to.tensor(MAPE_listY, device=device)).item()\n","    mean_MAPEX = to.mean(to.tensor(MAPE_listX, device=device)).item()\n","\n","    return mean_loss, mean_lossY, mean_lossX, mean_MAPEY, mean_MAPEX\n","\n","def valid_epoch(\n","    network,num_freq_in, num_freq_out,\n","    valid_dataloader,\n","    loss_function\n","    ):\n","\n","    loss_list = []\n","    loss_listY = []\n","    loss_listX = []\n","\n","    MAPE_listY = []\n","    MAPE_listX = []\n","\n","    for i, data in enumerate(valid_dataloader):\n","        X, Y = data\n","        predictY, predictX = network(X[:,0:num_freq_in])\n","\n","        lossY = loss_function(predictY, Y)\n","        lossX = loss_function(predictX, X)\n","        loss = lossY + 2*lossX\n","\n","        loss_listY.append(lossY.item())\n","        loss_listX.append(lossX.item())\n","        loss_list.append(loss.item())\n","        \n","        X[:,0:num_freq_out] = to.exp(X[:,0:num_freq_out])\n","        predictX[:,0:num_freq_out] = to.exp(predictX[:,0:num_freq_out])\n"," \n","        MAPEY = to.mean(to.abs((Y - predictY) / Y)*100)\n","        MAPEX = to.mean(to.abs((X - predictX) / X)*100)\n","\n","        MAPE_listY.append(MAPEY.item())\n","        MAPE_listX.append(MAPEX.item())\n","    \n","    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n","    mean_lossY = to.mean(to.tensor(loss_listY, device=device)).item()\n","    mean_lossX = to.mean(to.tensor(loss_listX, device=device)).item()\n","\n","    mean_MAPEY = to.mean(to.tensor(MAPE_listY, device=device)).item()\n","    mean_MAPEX = to.mean(to.tensor(MAPE_listX, device=device)).item()\n","\n","    return mean_loss, mean_lossY, mean_lossX, mean_MAPEY, mean_MAPEX"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"BV3w3s4kROnm"},"outputs":[],"source":["def train_GSAE(\n","    network, num_freq_in, num_freq_out,\n","    train_dataloader, valid_dataloader,\n","    loss_function, optimizer_type,\n","    epochs, learn_rate\n","    ):\n","\n","    if optimizer_type=='adam':\n","        optimizer = to.optim.Adam(network.parameters(), lr=learn_rate)\n","    else:\n","        optimizer = to.optim.SGD(network.parameters(), lr=learn_rate)\n","    \n","    tb_writer = sumwriter()\n","    \n","    for i in range(epochs):\n","        network.train(True)\n","        mloss, mlossY, mlossX, mMAPEY, mMAPEX = train_epoch(network, num_freq_in, num_freq_out, train_dataloader, loss_function, optimizer, tb_writer, i)\n","\n","        network.eval()\n","        with to.no_grad():\n","            vmloss, vmlossY, vmlossX, vmMAPEY, vmMAPEX = valid_epoch(network, num_freq_in, num_freq_out, valid_dataloader, loss_function)\n","\n","        print('-'*50)\n","        print('Epoch {} / {}'.format(i+1,epochs))\n","        print('-'*15)\n","        print('Average Train Loss : {}'.format(mloss))\n","        print('Average Validation Loss : {}'.format(vmloss))\n","\n","        tb_writer.add_scalars(\"Batch Mean Loss\",\n","                            {\n","                                'Train' : mloss,\n","                                'Validation' : vmloss\n","                            }, i+1)\n","\n","        tb_writer.add_scalars(\"Batch MAPE - Latent Space\",\n","                            {\n","                                'Train' : mMAPEY,\n","                                'Validation' : vmMAPEY\n","                            }, i+1)\n","\n","        tb_writer.add_scalars(\"Batch MAPE - Reconstruction\",\n","                            {\n","                                'Train' : mMAPEX,\n","                                'Validation' : vmMAPEX\n","                            }, i+1)\n","\n","        tb_writer.add_scalars(\"Batch Mean Loss - Latent Space\",\n","                            {\n","                                'Train' : mlossY,\n","                                'Validation' : vmlossY\n","                            }, i+1)\n","    \n","        tb_writer.add_scalars(\"Batch Mean Loss - Reconstruction\",\n","                            {\n","                                'Train' : mlossX,\n","                                'Validation' : vmlossX\n","                            }, i+1)\n","                            \n","    tb_writer.flush()\n","    tb_writer.close()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"4mGwGrY9ROnn"},"outputs":[],"source":["# Prepare Data\n","data = pd.read_csv('TrainValid_Data.csv')\n","data['psi'] =  12*data['rho']/ (data['E']*data['t']**2)\n","\n","num_freq_in = 4\n","num_freq_out = 10\n","\n","features = freq_name(num_freq_out,1,0)\n","latent = ['psi', 'nu', 'a', 'b']\n","\n","train_split = int(0.8*len(data))\n","valid_split = len(data)- train_split\n","\n","scaled_data = data[features+latent].copy()\n","\n","scaled_data['psi'] = np.log(scaled_data['psi']) \n","scaled_data[freq_name(num_freq_out,1,0)] = np.log(scaled_data[freq_name(num_freq_out,1,0)])\n","\n","scaled_data = GSAE_data(scaled_data, features, latent)\n","train_set, valid_set = to_data.random_split(scaled_data, [train_split, valid_split])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9329o3djROnn"},"outputs":[],"source":["# Train Model\n","# Parameters\n","num_features = len(features)\n","num_latent = len(latent)\n","he_nodes = [10,50,100,200,300,400,300,200,100,50,10]\n","hd_nodes = [10,50,100,200,300,400,300,200,100,50,10]\n","hactiv = 'prelu'\n","\n","batch_size_train = 2000\n","batch_size_valid = 2000\n","\n","epochs = 200\n","learn_rate = 1e-3\n","\n","# Optim Selections\n","loss_function = to.nn.SmoothL1Loss()\n","optimizer_type = 'adam'\n","\n","# Data loaders\n","train_loader = to.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True)\n","valid_loader = to.utils.data.DataLoader(valid_set, batch_size=batch_size_valid, shuffle=True)\n","\n","# Model\n","model = GSAE_Network(num_freq_in, num_latent, num_freq_out, he_nodes, hd_nodes, hactiv)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"cnO1IkVmROno","outputId":"0962fd2d-b856-415b-c04b-7676dbe0f8f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------------------------------------------\n","Epoch 1 / 200\n","---------------\n","Average Train Loss : 10.622230529785156\n","Average Validation Loss : 6.4720635414123535\n","--------------------------------------------------\n","Epoch 2 / 200\n","---------------\n","Average Train Loss : 4.804290771484375\n","Average Validation Loss : 3.064760208129883\n","--------------------------------------------------\n","Epoch 3 / 200\n","---------------\n","Average Train Loss : 1.6999393701553345\n","Average Validation Loss : 0.6723567843437195\n","--------------------------------------------------\n","Epoch 4 / 200\n","---------------\n","Average Train Loss : 0.3363741636276245\n","Average Validation Loss : 0.16747459769248962\n","--------------------------------------------------\n","Epoch 5 / 200\n","---------------\n","Average Train Loss : 0.17502914369106293\n","Average Validation Loss : 0.17657910287380219\n","--------------------------------------------------\n","Epoch 6 / 200\n","---------------\n","Average Train Loss : 0.1552240401506424\n","Average Validation Loss : 0.25859591364860535\n","--------------------------------------------------\n","Epoch 7 / 200\n","---------------\n","Average Train Loss : 0.1882871836423874\n","Average Validation Loss : 0.1766059398651123\n","--------------------------------------------------\n","Epoch 8 / 200\n","---------------\n","Average Train Loss : 0.17003123462200165\n","Average Validation Loss : 0.17502425611019135\n","--------------------------------------------------\n","Epoch 9 / 200\n","---------------\n","Average Train Loss : 0.17009294033050537\n","Average Validation Loss : 0.45903751254081726\n","--------------------------------------------------\n","Epoch 10 / 200\n","---------------\n","Average Train Loss : 0.21194128692150116\n","Average Validation Loss : 0.15368783473968506\n","--------------------------------------------------\n","Epoch 11 / 200\n","---------------\n","Average Train Loss : 0.13983286917209625\n","Average Validation Loss : 0.13075761497020721\n","--------------------------------------------------\n","Epoch 12 / 200\n","---------------\n","Average Train Loss : 0.16695116460323334\n","Average Validation Loss : 0.5456968545913696\n","--------------------------------------------------\n","Epoch 13 / 200\n","---------------\n","Average Train Loss : 0.2269391119480133\n","Average Validation Loss : 0.18543373048305511\n","--------------------------------------------------\n","Epoch 14 / 200\n","---------------\n","Average Train Loss : 0.14835786819458008\n","Average Validation Loss : 0.13469205796718597\n","--------------------------------------------------\n","Epoch 15 / 200\n","---------------\n","Average Train Loss : 0.13319069147109985\n","Average Validation Loss : 0.128665491938591\n","--------------------------------------------------\n","Epoch 16 / 200\n","---------------\n","Average Train Loss : 0.14398130774497986\n","Average Validation Loss : 0.37371382117271423\n","--------------------------------------------------\n","Epoch 17 / 200\n","---------------\n","Average Train Loss : 0.2670547068119049\n","Average Validation Loss : 0.18263249099254608\n","--------------------------------------------------\n","Epoch 18 / 200\n","---------------\n","Average Train Loss : 0.1434558480978012\n","Average Validation Loss : 0.1341165155172348\n","--------------------------------------------------\n","Epoch 19 / 200\n","---------------\n","Average Train Loss : 0.13494928181171417\n","Average Validation Loss : 0.13596788048744202\n","--------------------------------------------------\n","Epoch 20 / 200\n","---------------\n","Average Train Loss : 0.15029726922512054\n","Average Validation Loss : 0.2920551598072052\n","--------------------------------------------------\n","Epoch 21 / 200\n","---------------\n","Average Train Loss : 0.18486963212490082\n","Average Validation Loss : 0.1507152020931244\n","--------------------------------------------------\n","Epoch 22 / 200\n","---------------\n","Average Train Loss : 0.14211243391036987\n","Average Validation Loss : 0.13605795800685883\n","--------------------------------------------------\n","Epoch 23 / 200\n","---------------\n","Average Train Loss : 0.13779523968696594\n","Average Validation Loss : 0.18607299029827118\n","--------------------------------------------------\n","Epoch 24 / 200\n","---------------\n","Average Train Loss : 0.2031593769788742\n","Average Validation Loss : 0.13931681215763092\n","--------------------------------------------------\n","Epoch 25 / 200\n","---------------\n","Average Train Loss : 0.14130578935146332\n","Average Validation Loss : 0.13258858025074005\n","--------------------------------------------------\n","Epoch 26 / 200\n","---------------\n","Average Train Loss : 0.134229376912117\n","Average Validation Loss : 0.13919907808303833\n","--------------------------------------------------\n","Epoch 27 / 200\n","---------------\n","Average Train Loss : 0.2073441594839096\n","Average Validation Loss : 0.17549042403697968\n","--------------------------------------------------\n","Epoch 28 / 200\n","---------------\n","Average Train Loss : 0.14857864379882812\n","Average Validation Loss : 0.1250227689743042\n","--------------------------------------------------\n","Epoch 29 / 200\n","---------------\n","Average Train Loss : 0.13206255435943604\n","Average Validation Loss : 0.13877756893634796\n","--------------------------------------------------\n","Epoch 30 / 200\n","---------------\n","Average Train Loss : 0.17085838317871094\n","Average Validation Loss : 0.15463896095752716\n","--------------------------------------------------\n","Epoch 31 / 200\n","---------------\n","Average Train Loss : 0.15147142112255096\n","Average Validation Loss : 0.2267729789018631\n","--------------------------------------------------\n","Epoch 32 / 200\n","---------------\n","Average Train Loss : 0.14765238761901855\n","Average Validation Loss : 0.13012056052684784\n","--------------------------------------------------\n","Epoch 33 / 200\n","---------------\n","Average Train Loss : 0.2086619883775711\n","Average Validation Loss : 0.12735436856746674\n","--------------------------------------------------\n","Epoch 34 / 200\n","---------------\n","Average Train Loss : 0.13952670991420746\n","Average Validation Loss : 0.1291683167219162\n","--------------------------------------------------\n","Epoch 35 / 200\n","---------------\n","Average Train Loss : 0.12527208030223846\n","Average Validation Loss : 0.14062760770320892\n","--------------------------------------------------\n","Epoch 36 / 200\n","---------------\n","Average Train Loss : 0.22197680175304413\n","Average Validation Loss : 0.18821217119693756\n","--------------------------------------------------\n","Epoch 37 / 200\n","---------------\n","Average Train Loss : 0.14194300770759583\n","Average Validation Loss : 0.14562375843524933\n","--------------------------------------------------\n","Epoch 38 / 200\n","---------------\n","Average Train Loss : 0.12978939712047577\n","Average Validation Loss : 0.17564080655574799\n","--------------------------------------------------\n","Epoch 39 / 200\n","---------------\n","Average Train Loss : 0.13946524262428284\n","Average Validation Loss : 0.1221768781542778\n","--------------------------------------------------\n","Epoch 40 / 200\n","---------------\n","Average Train Loss : 0.18480300903320312\n","Average Validation Loss : 0.15653520822525024\n","--------------------------------------------------\n","Epoch 41 / 200\n","---------------\n","Average Train Loss : 0.18376106023788452\n","Average Validation Loss : 0.14707307517528534\n","--------------------------------------------------\n","Epoch 42 / 200\n","---------------\n","Average Train Loss : 0.13144968450069427\n","Average Validation Loss : 0.12199970334768295\n","--------------------------------------------------\n","Epoch 43 / 200\n","---------------\n","Average Train Loss : 0.1276029646396637\n","Average Validation Loss : 0.12847116589546204\n","--------------------------------------------------\n","Epoch 44 / 200\n","---------------\n","Average Train Loss : 0.16073398292064667\n","Average Validation Loss : 0.2958948612213135\n","--------------------------------------------------\n","Epoch 45 / 200\n","---------------\n","Average Train Loss : 0.15351150929927826\n","Average Validation Loss : 0.13149289786815643\n","--------------------------------------------------\n","Epoch 46 / 200\n","---------------\n","Average Train Loss : 0.12542346119880676\n","Average Validation Loss : 0.11591396480798721\n","--------------------------------------------------\n","Epoch 47 / 200\n","---------------\n","Average Train Loss : 0.15177002549171448\n","Average Validation Loss : 0.40718576312065125\n","--------------------------------------------------\n","Epoch 48 / 200\n","---------------\n","Average Train Loss : 0.1985599845647812\n","Average Validation Loss : 0.11379016935825348\n","--------------------------------------------------\n","Epoch 49 / 200\n","---------------\n","Average Train Loss : 0.12216098606586456\n","Average Validation Loss : 0.11262712627649307\n","--------------------------------------------------\n","Epoch 50 / 200\n","---------------\n","Average Train Loss : 0.13454096019268036\n","Average Validation Loss : 0.1287343055009842\n","--------------------------------------------------\n","Epoch 51 / 200\n","---------------\n","Average Train Loss : 0.12103557586669922\n","Average Validation Loss : 0.11473846435546875\n","--------------------------------------------------\n","Epoch 52 / 200\n","---------------\n","Average Train Loss : 0.12331344932317734\n","Average Validation Loss : 0.1163596510887146\n","--------------------------------------------------\n","Epoch 53 / 200\n","---------------\n","Average Train Loss : 0.1268194168806076\n","Average Validation Loss : 0.21684293448925018\n","--------------------------------------------------\n","Epoch 54 / 200\n","---------------\n","Average Train Loss : 0.1452159434556961\n","Average Validation Loss : 0.15852107107639313\n","--------------------------------------------------\n","Epoch 55 / 200\n","---------------\n","Average Train Loss : 0.1317346692085266\n","Average Validation Loss : 0.1494254320859909\n","--------------------------------------------------\n","Epoch 56 / 200\n","---------------\n","Average Train Loss : 0.1390516459941864\n","Average Validation Loss : 0.10507863759994507\n","--------------------------------------------------\n","Epoch 57 / 200\n","---------------\n","Average Train Loss : 0.2199680358171463\n","Average Validation Loss : 0.1594005823135376\n","--------------------------------------------------\n","Epoch 58 / 200\n","---------------\n","Average Train Loss : 0.13817879557609558\n","Average Validation Loss : 0.1097785010933876\n","--------------------------------------------------\n","Epoch 59 / 200\n","---------------\n","Average Train Loss : 0.11270391196012497\n","Average Validation Loss : 0.10039805620908737\n","--------------------------------------------------\n","Epoch 60 / 200\n","---------------\n","Average Train Loss : 0.140696719288826\n","Average Validation Loss : 0.09920208156108856\n","--------------------------------------------------\n","Epoch 61 / 200\n","---------------\n","Average Train Loss : 0.13015875220298767\n","Average Validation Loss : 0.1655435711145401\n","--------------------------------------------------\n","Epoch 62 / 200\n","---------------\n","Average Train Loss : 0.21728825569152832\n","Average Validation Loss : 0.19680796563625336\n","--------------------------------------------------\n","Epoch 63 / 200\n","---------------\n","Average Train Loss : 0.13274918496608734\n","Average Validation Loss : 0.10937787592411041\n","--------------------------------------------------\n","Epoch 64 / 200\n","---------------\n","Average Train Loss : 0.11559802293777466\n","Average Validation Loss : 0.1128479465842247\n","--------------------------------------------------\n","Epoch 65 / 200\n","---------------\n","Average Train Loss : 0.10869771242141724\n","Average Validation Loss : 0.13545934855937958\n","--------------------------------------------------\n","Epoch 66 / 200\n","---------------\n","Average Train Loss : 0.18342618644237518\n","Average Validation Loss : 0.14596129953861237\n","--------------------------------------------------\n","Epoch 67 / 200\n","---------------\n","Average Train Loss : 0.11456415802240372\n","Average Validation Loss : 0.10002882778644562\n","--------------------------------------------------\n","Epoch 68 / 200\n","---------------\n","Average Train Loss : 0.09675242751836777\n","Average Validation Loss : 0.09077450633049011\n","--------------------------------------------------\n","Epoch 69 / 200\n","---------------\n","Average Train Loss : 0.14062657952308655\n","Average Validation Loss : 0.12496413290500641\n","--------------------------------------------------\n","Epoch 70 / 200\n","---------------\n","Average Train Loss : 0.13977456092834473\n","Average Validation Loss : 0.11187555640935898\n","--------------------------------------------------\n","Epoch 71 / 200\n","---------------\n","Average Train Loss : 0.10782482475042343\n","Average Validation Loss : 0.08886656165122986\n","--------------------------------------------------\n","Epoch 72 / 200\n","---------------\n","Average Train Loss : 0.09727181494235992\n","Average Validation Loss : 0.09764063358306885\n","--------------------------------------------------\n","Epoch 73 / 200\n","---------------\n","Average Train Loss : 0.09718655794858932\n","Average Validation Loss : 0.108473040163517\n","--------------------------------------------------\n","Epoch 74 / 200\n","---------------\n","Average Train Loss : 0.12422799319028854\n","Average Validation Loss : 0.10755100101232529\n","--------------------------------------------------\n","Epoch 75 / 200\n","---------------\n","Average Train Loss : 0.09897526353597641\n","Average Validation Loss : 0.08456632494926453\n","--------------------------------------------------\n","Epoch 76 / 200\n","---------------\n","Average Train Loss : 0.09626046568155289\n","Average Validation Loss : 0.0929318219423294\n","--------------------------------------------------\n","Epoch 77 / 200\n","---------------\n","Average Train Loss : 0.10276762396097183\n","Average Validation Loss : 0.10672946274280548\n","--------------------------------------------------\n","Epoch 78 / 200\n","---------------\n","Average Train Loss : 0.10304161161184311\n","Average Validation Loss : 0.0834217444062233\n","--------------------------------------------------\n","Epoch 79 / 200\n","---------------\n","Average Train Loss : 0.11906295269727707\n","Average Validation Loss : 0.09658107161521912\n","--------------------------------------------------\n","Epoch 80 / 200\n","---------------\n","Average Train Loss : 0.16867652535438538\n","Average Validation Loss : 0.1127111092209816\n","--------------------------------------------------\n","Epoch 81 / 200\n","---------------\n","Average Train Loss : 0.10648862272500992\n","Average Validation Loss : 0.10750894993543625\n","--------------------------------------------------\n","Epoch 82 / 200\n","---------------\n","Average Train Loss : 0.1303662806749344\n","Average Validation Loss : 0.15344369411468506\n","--------------------------------------------------\n","Epoch 83 / 200\n","---------------\n","Average Train Loss : 0.11983232945203781\n","Average Validation Loss : 0.08762263506650925\n","--------------------------------------------------\n","Epoch 84 / 200\n","---------------\n","Average Train Loss : 0.09026272594928741\n","Average Validation Loss : 0.08569569140672684\n","--------------------------------------------------\n","Epoch 85 / 200\n","---------------\n","Average Train Loss : 0.08585720509290695\n","Average Validation Loss : 0.08781536668539047\n","--------------------------------------------------\n","Epoch 86 / 200\n","---------------\n","Average Train Loss : 0.09252555668354034\n","Average Validation Loss : 0.08760526031255722\n","--------------------------------------------------\n","Epoch 87 / 200\n","---------------\n","Average Train Loss : 0.08993949741125107\n","Average Validation Loss : 0.10529109090566635\n","--------------------------------------------------\n","Epoch 88 / 200\n","---------------\n","Average Train Loss : 0.0914284810423851\n","Average Validation Loss : 0.08629123121500015\n","--------------------------------------------------\n","Epoch 89 / 200\n","---------------\n","Average Train Loss : 0.10595660656690598\n","Average Validation Loss : 0.09133414179086685\n","--------------------------------------------------\n","Epoch 90 / 200\n","---------------\n","Average Train Loss : 0.0912608727812767\n","Average Validation Loss : 0.11309677362442017\n","--------------------------------------------------\n","Epoch 91 / 200\n","---------------\n","Average Train Loss : 0.09885776787996292\n","Average Validation Loss : 0.08489101380109787\n","--------------------------------------------------\n","Epoch 92 / 200\n","---------------\n","Average Train Loss : 0.1039964109659195\n","Average Validation Loss : 0.13068264722824097\n","--------------------------------------------------\n","Epoch 93 / 200\n","---------------\n","Average Train Loss : 0.08943285793066025\n","Average Validation Loss : 0.08038736879825592\n","--------------------------------------------------\n","Epoch 94 / 200\n","---------------\n","Average Train Loss : 0.14692339301109314\n","Average Validation Loss : 0.41637998819351196\n","--------------------------------------------------\n","Epoch 95 / 200\n","---------------\n","Average Train Loss : 0.7389830946922302\n","Average Validation Loss : 0.25032711029052734\n","--------------------------------------------------\n","Epoch 96 / 200\n","---------------\n","Average Train Loss : 0.6076175570487976\n","Average Validation Loss : 0.17891733348369598\n","--------------------------------------------------\n","Epoch 97 / 200\n","---------------\n","Average Train Loss : 0.1654486507177353\n","Average Validation Loss : 0.11549365520477295\n","--------------------------------------------------\n","Epoch 98 / 200\n","---------------\n","Average Train Loss : 0.10260913521051407\n","Average Validation Loss : 0.09028059989213943\n","--------------------------------------------------\n","Epoch 99 / 200\n","---------------\n","Average Train Loss : 0.09136753529310226\n","Average Validation Loss : 0.08834158629179001\n","--------------------------------------------------\n","Epoch 100 / 200\n","---------------\n","Average Train Loss : 0.08775755017995834\n","Average Validation Loss : 0.08624853938817978\n","--------------------------------------------------\n","Epoch 101 / 200\n","---------------\n","Average Train Loss : 0.08707156032323837\n","Average Validation Loss : 0.08972930908203125\n","--------------------------------------------------\n","Epoch 102 / 200\n","---------------\n","Average Train Loss : 0.08652777969837189\n","Average Validation Loss : 0.08441279828548431\n","--------------------------------------------------\n","Epoch 103 / 200\n","---------------\n","Average Train Loss : 0.08496164530515671\n","Average Validation Loss : 0.08377279341220856\n","--------------------------------------------------\n","Epoch 104 / 200\n","---------------\n","Average Train Loss : 0.08511731773614883\n","Average Validation Loss : 0.08630763739347458\n","--------------------------------------------------\n","Epoch 105 / 200\n","---------------\n","Average Train Loss : 0.08540178835391998\n","Average Validation Loss : 0.09740393608808517\n","--------------------------------------------------\n","Epoch 106 / 200\n","---------------\n","Average Train Loss : 0.09142515063285828\n","Average Validation Loss : 0.09015475958585739\n","--------------------------------------------------\n","Epoch 107 / 200\n","---------------\n","Average Train Loss : 0.0859871581196785\n","Average Validation Loss : 0.0840277373790741\n","--------------------------------------------------\n","Epoch 108 / 200\n","---------------\n","Average Train Loss : 0.08370422571897507\n","Average Validation Loss : 0.0819728896021843\n","--------------------------------------------------\n","Epoch 109 / 200\n","---------------\n","Average Train Loss : 0.08260098844766617\n","Average Validation Loss : 0.08166348189115524\n","--------------------------------------------------\n","Epoch 110 / 200\n","---------------\n","Average Train Loss : 0.08794277161359787\n","Average Validation Loss : 0.09357917308807373\n","--------------------------------------------------\n","Epoch 111 / 200\n","---------------\n","Average Train Loss : 0.09593596309423447\n","Average Validation Loss : 0.09708040207624435\n","--------------------------------------------------\n","Epoch 112 / 200\n","---------------\n","Average Train Loss : 0.08814265578985214\n","Average Validation Loss : 0.08606570214033127\n","--------------------------------------------------\n","Epoch 113 / 200\n","---------------\n","Average Train Loss : 0.08454518765211105\n","Average Validation Loss : 0.08491666615009308\n","--------------------------------------------------\n","Epoch 114 / 200\n","---------------\n","Average Train Loss : 0.08318274468183517\n","Average Validation Loss : 0.08147497475147247\n","--------------------------------------------------\n","Epoch 115 / 200\n","---------------\n","Average Train Loss : 0.0842977836728096\n","Average Validation Loss : 0.08425115793943405\n","--------------------------------------------------\n","Epoch 116 / 200\n","---------------\n","Average Train Loss : 0.0894075557589531\n","Average Validation Loss : 0.08034463971853256\n","--------------------------------------------------\n","Epoch 117 / 200\n","---------------\n","Average Train Loss : 0.08365997672080994\n","Average Validation Loss : 0.0851951465010643\n","--------------------------------------------------\n","Epoch 118 / 200\n","---------------\n","Average Train Loss : 0.0895211324095726\n","Average Validation Loss : 0.08512186259031296\n","--------------------------------------------------\n","Epoch 119 / 200\n","---------------\n","Average Train Loss : 0.08175139129161835\n","Average Validation Loss : 0.08109338581562042\n","--------------------------------------------------\n","Epoch 120 / 200\n","---------------\n","Average Train Loss : 0.10142900794744492\n","Average Validation Loss : 0.08063657581806183\n","--------------------------------------------------\n","Epoch 121 / 200\n","---------------\n","Average Train Loss : 0.0820726752281189\n","Average Validation Loss : 0.08020573854446411\n","--------------------------------------------------\n","Epoch 122 / 200\n","---------------\n","Average Train Loss : 0.12429407984018326\n","Average Validation Loss : 0.2125181257724762\n","--------------------------------------------------\n","Epoch 123 / 200\n","---------------\n","Average Train Loss : 0.1555209457874298\n","Average Validation Loss : 0.1383984535932541\n","--------------------------------------------------\n","Epoch 124 / 200\n","---------------\n","Average Train Loss : 0.11573570966720581\n","Average Validation Loss : 0.10419856756925583\n","--------------------------------------------------\n","Epoch 125 / 200\n","---------------\n","Average Train Loss : 0.10476618260145187\n","Average Validation Loss : 0.09895350784063339\n","--------------------------------------------------\n","Epoch 126 / 200\n","---------------\n","Average Train Loss : 0.14445596933364868\n","Average Validation Loss : 0.1700570434331894\n","--------------------------------------------------\n","Epoch 127 / 200\n","---------------\n","Average Train Loss : 0.11980319023132324\n","Average Validation Loss : 0.10194939374923706\n","--------------------------------------------------\n","Epoch 128 / 200\n","---------------\n","Average Train Loss : 0.09814510494470596\n","Average Validation Loss : 0.08882849663496017\n","--------------------------------------------------\n","Epoch 129 / 200\n","---------------\n","Average Train Loss : 0.09441033005714417\n","Average Validation Loss : 0.0891609713435173\n","--------------------------------------------------\n","Epoch 130 / 200\n","---------------\n","Average Train Loss : 0.08556025475263596\n","Average Validation Loss : 0.08279456198215485\n","--------------------------------------------------\n","Epoch 131 / 200\n","---------------\n","Average Train Loss : 0.08328038454055786\n","Average Validation Loss : 0.08226532489061356\n","--------------------------------------------------\n","Epoch 132 / 200\n","---------------\n","Average Train Loss : 0.0828864648938179\n","Average Validation Loss : 0.08230612426996231\n","--------------------------------------------------\n","Epoch 133 / 200\n","---------------\n","Average Train Loss : 0.08216066658496857\n","Average Validation Loss : 0.08130893111228943\n","--------------------------------------------------\n","Epoch 134 / 200\n","---------------\n","Average Train Loss : 0.08278802782297134\n","Average Validation Loss : 0.08379270881414413\n","--------------------------------------------------\n","Epoch 135 / 200\n","---------------\n","Average Train Loss : 0.09740737825632095\n","Average Validation Loss : 0.09179643541574478\n","--------------------------------------------------\n","Epoch 136 / 200\n","---------------\n","Average Train Loss : 0.08422748744487762\n","Average Validation Loss : 0.08257941156625748\n","--------------------------------------------------\n","Epoch 137 / 200\n","---------------\n","Average Train Loss : 0.08207820355892181\n","Average Validation Loss : 0.09192216396331787\n","--------------------------------------------------\n","Epoch 138 / 200\n","---------------\n","Average Train Loss : 0.1062585860490799\n","Average Validation Loss : 0.08783935010433197\n","--------------------------------------------------\n","Epoch 139 / 200\n","---------------\n","Average Train Loss : 0.08397891372442245\n","Average Validation Loss : 0.08233106136322021\n","--------------------------------------------------\n","Epoch 140 / 200\n","---------------\n","Average Train Loss : 0.08753440529108047\n","Average Validation Loss : 0.08112895488739014\n","--------------------------------------------------\n","Epoch 141 / 200\n","---------------\n","Average Train Loss : 0.10498218983411789\n","Average Validation Loss : 0.12284374237060547\n","--------------------------------------------------\n","Epoch 142 / 200\n","---------------\n","Average Train Loss : 0.09229041635990143\n","Average Validation Loss : 0.08193380385637283\n","--------------------------------------------------\n","Epoch 143 / 200\n","---------------\n","Average Train Loss : 0.07987377047538757\n","Average Validation Loss : 0.0786062628030777\n","--------------------------------------------------\n","Epoch 144 / 200\n","---------------\n","Average Train Loss : 0.18910230696201324\n","Average Validation Loss : 0.1201268658041954\n","--------------------------------------------------\n","Epoch 145 / 200\n","---------------\n","Average Train Loss : 0.16329847276210785\n","Average Validation Loss : 0.156253382563591\n","--------------------------------------------------\n","Epoch 146 / 200\n","---------------\n","Average Train Loss : 0.10999792814254761\n","Average Validation Loss : 0.09568607807159424\n","--------------------------------------------------\n","Epoch 147 / 200\n","---------------\n","Average Train Loss : 0.08674933761358261\n","Average Validation Loss : 0.08287209272384644\n","--------------------------------------------------\n","Epoch 148 / 200\n","---------------\n","Average Train Loss : 0.08250739425420761\n","Average Validation Loss : 0.08147887140512466\n","--------------------------------------------------\n","Epoch 149 / 200\n","---------------\n","Average Train Loss : 0.08211614936590195\n","Average Validation Loss : 0.08240362256765366\n","--------------------------------------------------\n","Epoch 150 / 200\n","---------------\n","Average Train Loss : 0.08155664056539536\n","Average Validation Loss : 0.08057478815317154\n","--------------------------------------------------\n","Epoch 151 / 200\n","---------------\n","Average Train Loss : 0.08128904551267624\n","Average Validation Loss : 0.08032133430242538\n","--------------------------------------------------\n","Epoch 152 / 200\n","---------------\n","Average Train Loss : 0.080854631960392\n","Average Validation Loss : 0.08018841594457626\n","--------------------------------------------------\n","Epoch 153 / 200\n","---------------\n","Average Train Loss : 0.088298000395298\n","Average Validation Loss : 0.1623208373785019\n","--------------------------------------------------\n","Epoch 154 / 200\n","---------------\n","Average Train Loss : 0.1400926113128662\n","Average Validation Loss : 0.12818902730941772\n","--------------------------------------------------\n","Epoch 155 / 200\n","---------------\n","Average Train Loss : 0.10473569482564926\n","Average Validation Loss : 0.08283790946006775\n","--------------------------------------------------\n","Epoch 156 / 200\n","---------------\n","Average Train Loss : 0.08620923012495041\n","Average Validation Loss : 0.0827350839972496\n","--------------------------------------------------\n","Epoch 157 / 200\n","---------------\n","Average Train Loss : 0.08191808313131332\n","Average Validation Loss : 0.08001101016998291\n","--------------------------------------------------\n","Epoch 158 / 200\n","---------------\n","Average Train Loss : 0.08110274374485016\n","Average Validation Loss : 0.07915531098842621\n","--------------------------------------------------\n","Epoch 159 / 200\n","---------------\n","Average Train Loss : 0.08245033025741577\n","Average Validation Loss : 0.09334617108106613\n","--------------------------------------------------\n","Epoch 160 / 200\n","---------------\n","Average Train Loss : 0.10963918268680573\n","Average Validation Loss : 0.08491703122854233\n","--------------------------------------------------\n","Epoch 161 / 200\n","---------------\n","Average Train Loss : 0.08536969870328903\n","Average Validation Loss : 0.08282829076051712\n","--------------------------------------------------\n","Epoch 162 / 200\n","---------------\n","Average Train Loss : 0.08315848559141159\n","Average Validation Loss : 0.13124653697013855\n","--------------------------------------------------\n","Epoch 163 / 200\n","---------------\n","Average Train Loss : 0.12552081048488617\n","Average Validation Loss : 0.08837689459323883\n","--------------------------------------------------\n","Epoch 164 / 200\n","---------------\n","Average Train Loss : 0.08305530995130539\n","Average Validation Loss : 0.0795191302895546\n","--------------------------------------------------\n","Epoch 165 / 200\n","---------------\n","Average Train Loss : 0.08724281936883926\n","Average Validation Loss : 0.11325707286596298\n","--------------------------------------------------\n","Epoch 166 / 200\n","---------------\n","Average Train Loss : 0.09818271547555923\n","Average Validation Loss : 0.0797179713845253\n","--------------------------------------------------\n","Epoch 167 / 200\n","---------------\n","Average Train Loss : 0.0793914720416069\n","Average Validation Loss : 0.07869450002908707\n","--------------------------------------------------\n","Epoch 168 / 200\n","---------------\n","Average Train Loss : 0.11768770217895508\n","Average Validation Loss : 0.11283550411462784\n","--------------------------------------------------\n","Epoch 169 / 200\n","---------------\n","Average Train Loss : 0.09024527668952942\n","Average Validation Loss : 0.07761881500482559\n","--------------------------------------------------\n","Epoch 170 / 200\n","---------------\n","Average Train Loss : 0.0786915123462677\n","Average Validation Loss : 0.0767807736992836\n","--------------------------------------------------\n","Epoch 171 / 200\n","---------------\n","Average Train Loss : 0.11377459764480591\n","Average Validation Loss : 0.4208745062351227\n","--------------------------------------------------\n","Epoch 172 / 200\n","---------------\n","Average Train Loss : 0.2677699029445648\n","Average Validation Loss : 0.2267441600561142\n","--------------------------------------------------\n","Epoch 173 / 200\n","---------------\n","Average Train Loss : 0.16463176906108856\n","Average Validation Loss : 0.12253379821777344\n","--------------------------------------------------\n","Epoch 174 / 200\n","---------------\n","Average Train Loss : 0.10444533079862595\n","Average Validation Loss : 0.10039950907230377\n","--------------------------------------------------\n","Epoch 175 / 200\n","---------------\n","Average Train Loss : 0.0892942026257515\n","Average Validation Loss : 0.08390236645936966\n","--------------------------------------------------\n","Epoch 176 / 200\n","---------------\n","Average Train Loss : 0.09359347075223923\n","Average Validation Loss : 0.11244680732488632\n","--------------------------------------------------\n","Epoch 177 / 200\n","---------------\n","Average Train Loss : 0.09058498591184616\n","Average Validation Loss : 0.08344908803701401\n","--------------------------------------------------\n","Epoch 178 / 200\n","---------------\n","Average Train Loss : 0.08256161957979202\n","Average Validation Loss : 0.08212225139141083\n","--------------------------------------------------\n","Epoch 179 / 200\n","---------------\n","Average Train Loss : 0.08175510913133621\n","Average Validation Loss : 0.08078309148550034\n","--------------------------------------------------\n","Epoch 180 / 200\n","---------------\n","Average Train Loss : 0.08229515701532364\n","Average Validation Loss : 0.08308948576450348\n","--------------------------------------------------\n","Epoch 181 / 200\n","---------------\n","Average Train Loss : 0.08141260594129562\n","Average Validation Loss : 0.08607330173254013\n","--------------------------------------------------\n","Epoch 182 / 200\n","---------------\n","Average Train Loss : 0.09715979546308517\n","Average Validation Loss : 0.07919096201658249\n","--------------------------------------------------\n","Epoch 183 / 200\n","---------------\n","Average Train Loss : 0.0839671939611435\n","Average Validation Loss : 0.07855530828237534\n","--------------------------------------------------\n","Epoch 184 / 200\n","---------------\n","Average Train Loss : 0.0867592915892601\n","Average Validation Loss : 0.0784030482172966\n","--------------------------------------------------\n","Epoch 185 / 200\n","---------------\n","Average Train Loss : 0.08728185296058655\n","Average Validation Loss : 0.07772593200206757\n","--------------------------------------------------\n","Epoch 186 / 200\n","---------------\n","Average Train Loss : 0.1014556884765625\n","Average Validation Loss : 0.09387388080358505\n","--------------------------------------------------\n","Epoch 187 / 200\n","---------------\n","Average Train Loss : 0.09151940792798996\n","Average Validation Loss : 0.0849006325006485\n","--------------------------------------------------\n","Epoch 188 / 200\n","---------------\n","Average Train Loss : 0.08388841897249222\n","Average Validation Loss : 0.08931036293506622\n","--------------------------------------------------\n","Epoch 189 / 200\n","---------------\n","Average Train Loss : 0.09171253442764282\n","Average Validation Loss : 0.08293155580759048\n","--------------------------------------------------\n","Epoch 190 / 200\n","---------------\n","Average Train Loss : 0.08317507803440094\n","Average Validation Loss : 0.12784020602703094\n","--------------------------------------------------\n","Epoch 191 / 200\n","---------------\n","Average Train Loss : 0.11481814831495285\n","Average Validation Loss : 0.08449865877628326\n","--------------------------------------------------\n","Epoch 192 / 200\n","---------------\n","Average Train Loss : 0.08413142710924149\n","Average Validation Loss : 0.07990926504135132\n","--------------------------------------------------\n","Epoch 193 / 200\n","---------------\n","Average Train Loss : 0.08128193765878677\n","Average Validation Loss : 0.14529748260974884\n","--------------------------------------------------\n","Epoch 194 / 200\n","---------------\n","Average Train Loss : 0.11744659394025803\n","Average Validation Loss : 0.08367086946964264\n","--------------------------------------------------\n","Epoch 195 / 200\n","---------------\n","Average Train Loss : 0.08107046037912369\n","Average Validation Loss : 0.07676402479410172\n","--------------------------------------------------\n","Epoch 196 / 200\n","---------------\n","Average Train Loss : 0.08828463405370712\n","Average Validation Loss : 0.19090178608894348\n","--------------------------------------------------\n","Epoch 197 / 200\n","---------------\n","Average Train Loss : 0.10807134211063385\n","Average Validation Loss : 0.0769505426287651\n","--------------------------------------------------\n","Epoch 198 / 200\n","---------------\n","Average Train Loss : 0.08043514937162399\n","Average Validation Loss : 0.07683251053094864\n","--------------------------------------------------\n","Epoch 199 / 200\n","---------------\n","Average Train Loss : 0.10540544241666794\n","Average Validation Loss : 0.08269041031599045\n","--------------------------------------------------\n","Epoch 200 / 200\n","---------------\n","Average Train Loss : 0.09269226342439651\n","Average Validation Loss : 0.08358567208051682\n"]}],"source":["# Train\n","train_GSAE(\n","    model, num_freq_in, num_freq_out,\n","    train_loader, valid_loader,\n","    loss_function, optimizer_type,\n","    epochs, learn_rate)\n","\n","to.save(model.state_dict(), 'GSAE_model.state')"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"F460CQdzROno"},"outputs":[{"name":"stdout","output_type":"stream","text":["Absolute Percentage Errors: \n","--------------------\n","D : 210.67 %\n","m_A : 32.16 %\n","rho : 25.66 %\n","a : 61.65 %\n"]}],"source":["model.load_state_dict(to.load('GSAE_model.state'))\n","\n","test_data = pd.read_csv('Test_Data.csv')\n","test_data['psi'] =  12*test_data['rho']/ (test_data['E']*test_data['t']**2)\n","\n","train_split = int(0.8*len(test_data))\n","valid_split = len(test_data)- train_split\n","\n","scaled_test_data = test_data[features+latent].copy()\n","\n","scaled_test_data['psi'] = np.log(scaled_test_data['psi'])\n","scaled_test_data[freq_name(num_freq_out,1,0)] = np.log(scaled_test_data[freq_name(num_freq_out,1,0)])\n","\n","scaled_test_data = GSAE_data(scaled_test_data, features, latent)\n","test_loader = to.utils.data.DataLoader(scaled_test_data, batch_size=len(scaled_test_data), shuffle=False)\n","\n","model.eval()\n","with to.no_grad():\n","    for i, data in enumerate(test_loader):\n","        X, Y = data\n","        predictY = model.encoder(X[:,0:num_freq_in])\n","        Y[:,0] = to.exp(Y[:,0])\n","        predictY[:,0] = to.exp(predictY[:,0])\n","        abs_perc_error = to.abs((Y - predictY)/Y)*100\n","        MAPE_per_dim = to.mean(abs_perc_error, 0)\n","\n","        print('Absolute Percentage Errors: ')\n","        print('-'*20)\n","        print('D : {:0.2f} %'.format(MAPE_per_dim[0].item()))\n","        print('m_A : {:0.2f} %'.format(MAPE_per_dim[1].item()))\n","        print('rho : {:0.2f} %'.format(MAPE_per_dim[2].item()))\n","        print('a : {:0.2f} %'.format(MAPE_per_dim[3].item()))\n","        # print('b : {:0.2f} %'.format(MAPE_per_dim[4].item()))\n","        # print('t : {:0.2f} %'.format(MAPE_per_dim[5].item()))\n","\n","        # plt.figure(figsize=(10,10))\n","        # plt.plot(['E', r'$\\nu$', r'$\\rho$', 'a', 'b', 't'], MAPE_per_dim.cpu(), 'b:o')\n","        # plt.ylim(0,100)\n","        # plt.grid(axis='y')\n","        # plt.yticks(np.arange(0,105,5))\n","        # plt.xticks(fontsize=16)\n","        # plt.title('GSAE MAPE Plot', fontsize=20)\n","        # plt.ylabel('MAPE [%]', fontsize=18)\n","        # plt.xlabel('Parameter', fontsize=18)\n","        # plt.savefig('GSAE_MAPE_plot.pdf', dpi=1200, bbox_inches='tight')\n","        # plt.show()\n","\n","        np.savetxt('GSAE_MAPE_test.txt', MAPE_per_dim.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"2426e189533ac41f5aad4fbc5dc1b573401ba6ae3fed99d7f2363912ea400c87"}}},"nbformat":4,"nbformat_minor":0}
