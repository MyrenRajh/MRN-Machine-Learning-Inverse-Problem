{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import torch as to\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import torch.utils.data as to_data\n",
    "from torch.utils.tensorboard import SummaryWriter as sumwriter\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Specify hardware for ML training (GPU default)\n",
    "device = \"cuda\" if to.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly generate list of strings for frequency numbers and ratios\n",
    "def freq_name(no_freq, include_freq=True, include_ratio=True):\n",
    "    \"\"\"\n",
    "    Creates an ordered list of string from inputted parameters:\n",
    "\n",
    "    no_freq = (int) number of desired frequencies\n",
    "    include_freq = (bool) include the individual frequencies or not (default True)\n",
    "    include_ratio = (bool) include the non-trivial ratios between frequencies or not (default True)\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    if include_freq:\n",
    "        for i in range(no_freq):\n",
    "            names.append('f'+str(i+1))\n",
    "    if include_ratio:\n",
    "        for i in range(no_freq):\n",
    "            for j in range(i):\n",
    "                names.append('f'+str(i+1)+'/f'+str(j+1))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(activ_name):\n",
    "    if activ_name=='relu':\n",
    "        return to.nn.ReLU()\n",
    "    elif activ_name=='lrelu':\n",
    "        return to.nn.LeakyReLU()\n",
    "    elif activ_name=='prelu':\n",
    "        return to.nn.PReLU()\n",
    "    elif activ_name=='relu6':\n",
    "        return to.nn.ReLU6()\n",
    "    elif activ_name=='sigmoid':\n",
    "        return to.nn.Sigmoid()\n",
    "    elif activ_name=='tanh':\n",
    "        return to.nn.Tanh()\n",
    "    elif activ_name=='silu':\n",
    "        return to.nn.SiLU()\n",
    "    elif activ_name=='selu':\n",
    "        return to.nn.SELU()\n",
    "    elif activ_name=='celu':\n",
    "        return to.nn.CELU()\n",
    "    elif activ_name=='gelu':\n",
    "        return to.nn.GELU()\n",
    "    else:\n",
    "        return to.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE_data(to_data.Dataset):\n",
    "    def __init__(self, scaled_dataframe, X_names, Y_names):\n",
    "        self.len = len(scaled_dataframe)\n",
    "        self.X = to.from_numpy(scaled_dataframe[X_names].to_numpy().astype('float32')).to(device)\n",
    "        self.Y = to.from_numpy(scaled_dataframe[Y_names].to_numpy().astype('float32')).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        X_idx = self.X[idx,:]\n",
    "        Y_idx = self.Y[idx,:]\n",
    "        return X_idx, Y_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE_Network(to.nn.Module):\n",
    "    def __init__(self, num_X, num_Y, num_Z, he_nodes, hd_nodes, hactiv_type):\n",
    "        super(SAE_Network, self).__init__()\n",
    "\n",
    "        self.esec = []\n",
    "        self.esec.append(to.nn.Linear(num_X+num_Y, he_nodes[0]))\n",
    "        self.esec.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(he_nodes)-1):\n",
    "            self.esec.append(to.nn.Linear(he_nodes[i], he_nodes[i+1]))\n",
    "            self.esec.append(activation(hactiv_type))\n",
    "\n",
    "        self.esec.append(to.nn.Linear(he_nodes[-1], 2*num_Z))\n",
    "\n",
    "        self.esec = to.nn.Sequential(*self.esec).to(device)\n",
    "        for i in self.esec[::2]:\n",
    "            to.nn.init.kaiming_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "        self.mu_layer = to.nn.Linear(2*num_Z, num_Z).to(device)\n",
    "        to.nn.init.kaiming_uniform_(self.mu_layer.weight)\n",
    "        to.nn.init.zeros_(self.mu_layer.bias)\n",
    "\n",
    "        self.var_layer = to.nn.Linear(2*num_Z, num_Z).to(device)\n",
    "        to.nn.init.kaiming_uniform_(self.var_layer.weight)\n",
    "        to.nn.init.zeros_(self.var_layer.bias)\n",
    "\n",
    "\n",
    "        self.dsec = []\n",
    "        self.dsec.append(to.nn.Linear(num_X+num_Z, hd_nodes[0]))\n",
    "        self.dsec.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(hd_nodes)-1):\n",
    "            self.dsec.append(to.nn.Linear(hd_nodes[i], hd_nodes[i+1]))\n",
    "            self.dsec.append(activation(hactiv_type))\n",
    "\n",
    "        self.dsec.append(to.nn.Linear(hd_nodes[-1], num_Y))\n",
    "        \n",
    "        self.dsec = to.nn.Sequential(*self.dsec).to(device)\n",
    "        for i in self.dsec[::2]:\n",
    "            to.nn.init.kaiming_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = to.exp(0.5*logvar)\n",
    "        eps = to.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def encode(self, X, Y):\n",
    "        inputs = to.cat([Y, X], 1)\n",
    "        Zp = self.esec(inputs)\n",
    "        Z_mu = self.mu_layer(Zp)\n",
    "        Z_var = self.var_layer(Zp)\n",
    "        return Z_mu, Z_var\n",
    "    \n",
    "    def decode(self, X, Z): # P(x|z, c)\n",
    "        inputs = to.cat([Z, X], 1) # (bs, latent_size+class_size)\n",
    "        Yp = self.dsec(inputs)\n",
    "        return Yp\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        Z_mu, Z_logvar = self.encode(X, Y)\n",
    "        Z = self.reparameterize(Z_mu, Z_logvar)\n",
    "        Yp = self.decode(X, Z)\n",
    "        return Yp, Z_mu, Z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network, num_freq,\n",
    "    train_dataloader,\n",
    "    loss_function, optimizer,\n",
    "    tb_writer, epoch_ind\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        X, Y = data\n",
    "\n",
    "        # if epoch_ind==0 and i==0:\n",
    "        #     tb_writer.add_graph(network, (Y,X), verbose=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        Yp, Z_mu, Z_logvar = network(X, Y)\n",
    "\n",
    "        loss = loss_function(Yp, Y) -0.5 * to.sum(1 + Z_logvar - Z_mu.pow(2) - Z_logvar.exp())\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "def valid_epoch(\n",
    "    network, num_freq,\n",
    "    valid_dataloader,\n",
    "    loss_function\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        X, Y = data\n",
    "        Yp, Z_mu, Z_logvar = network(X, Y)\n",
    "        loss = loss_function(Yp, Y) -0.5 * to.sum(1 + Z_logvar - Z_mu.pow(2) - Z_logvar.exp())\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AEI(\n",
    "    network, num_freq,\n",
    "    train_dataloader, valid_dataloader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate\n",
    "    ):\n",
    "\n",
    "    if optimizer_type=='adam':\n",
    "        optimizer = to.optim.Adam(network.parameters(), lr=learn_rate)\n",
    "    else:\n",
    "        optimizer = to.optim.SGD(network.parameters(), lr=learn_rate)\n",
    "    \n",
    "    tb_writer = sumwriter('Current_ML_Results/Tensorboard')\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        network.train(True)\n",
    "        mloss = train_epoch(network, num_freq, train_dataloader, loss_function, optimizer, tb_writer, i)\n",
    "\n",
    "        network.eval()\n",
    "        with to.no_grad():\n",
    "            vmloss = valid_epoch(network, num_freq, valid_dataloader, loss_function)\n",
    "        \n",
    "\n",
    "        print('-'*50)\n",
    "        print('Epoch {} / {}'.format(i+1,epochs))\n",
    "        print('-'*15)\n",
    "        print('Average Train Loss : {}'.format(mloss))\n",
    "        print('Average Validation Loss : {}'.format(vmloss))\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch Mean Loss\",\n",
    "                            {\n",
    "                                'Train' : mloss,\n",
    "                                'Validation' : vmloss\n",
    "                            }, i+1)\n",
    "                            \n",
    "    tb_writer.flush()\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "import sklearn.preprocessing as sk_preproc\n",
    "data = pd.read_csv('Data Workspace\\FM_TV_Data.csv')\n",
    "num_freq = 6\n",
    "\n",
    "X_names = freq_name(num_freq,1,1)\n",
    "Y_names = ['c', 'nu', 'a', 'b', 't']\n",
    "\n",
    "train_split = int(0.8*len(data))\n",
    "valid_split = len(data)- train_split\n",
    "\n",
    "scaled_data = data[X_names+Y_names].copy()\n",
    "\n",
    "# scaled_data['psi'] = np.log(scaled_data['psi'])\n",
    "# scaled_data['E'] = scaled_data['E']/1e11\n",
    "# scaled_data['rho'] = scaled_data['rho']/10000\n",
    "# scaled_data['t'] = scaled_data['t']*100\n",
    "scaled_data[freq_name(num_freq,1,0)] = np.log(scaled_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scalerX = sk_preproc.StandardScaler()\n",
    "scalerY = sk_preproc.StandardScaler()\n",
    "scalerX.fit(scaled_data[X_names])\n",
    "scalerY.fit(scaled_data[Y_names])\n",
    "\n",
    "scaled_data[X_names]= scalerX.transform(scaled_data[X_names])\n",
    "scaled_data[Y_names]= scalerY.transform(scaled_data[Y_names])\n",
    "\n",
    "scaled_data = SAE_data(scaled_data, X_names, Y_names)\n",
    "train_set, valid_set = to_data.random_split(scaled_data, [train_split, valid_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "# Parameters\n",
    "num_X = len(X_names)\n",
    "num_Y = len(Y_names)\n",
    "num_Z = 10\n",
    "he_nodes = [60,55,50,45,40,35,30,25,20]\n",
    "hd_nodes = [60,55,50,45,40,35,30,25,20][::-1]\n",
    "hactiv = 'silu'\n",
    "\n",
    "batch_size_train = 200\n",
    "batch_size_valid = 2000\n",
    "\n",
    "epochs = 100\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# Optim Selections\n",
    "loss_function = to.nn.SmoothL1Loss()\n",
    "optimizer_type = 'adam'\n",
    "\n",
    "# Data loaders\n",
    "train_loader = to.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True)\n",
    "valid_loader = to.utils.data.DataLoader(valid_set, batch_size=batch_size_valid, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = SAE_Network(num_X, num_Y, num_Z, he_nodes, hd_nodes, hactiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Epoch 1 / 100\n",
      "---------------\n",
      "Average Train Loss : 16.293413162231445\n",
      "Average Validation Loss : 8.602920532226562\n",
      "--------------------------------------------------\n",
      "Epoch 2 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.8078553080558777\n",
      "Average Validation Loss : 3.883915901184082\n",
      "--------------------------------------------------\n",
      "Epoch 3 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.5203116536140442\n",
      "Average Validation Loss : 2.370163917541504\n",
      "--------------------------------------------------\n",
      "Epoch 4 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.40549397468566895\n",
      "Average Validation Loss : 1.6446778774261475\n",
      "--------------------------------------------------\n",
      "Epoch 5 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.33970922231674194\n",
      "Average Validation Loss : 1.2370668649673462\n",
      "--------------------------------------------------\n",
      "Epoch 6 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.30110207200050354\n",
      "Average Validation Loss : 0.9787810444831848\n",
      "--------------------------------------------------\n",
      "Epoch 7 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.27263519167900085\n",
      "Average Validation Loss : 0.8044281005859375\n",
      "--------------------------------------------------\n",
      "Epoch 8 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.2519007623195648\n",
      "Average Validation Loss : 0.6807940006256104\n",
      "--------------------------------------------------\n",
      "Epoch 9 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.23768535256385803\n",
      "Average Validation Loss : 0.589583158493042\n",
      "--------------------------------------------------\n",
      "Epoch 10 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.22769193351268768\n",
      "Average Validation Loss : 0.5234280824661255\n",
      "--------------------------------------------------\n",
      "Epoch 11 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.21983659267425537\n",
      "Average Validation Loss : 0.47097378969192505\n",
      "--------------------------------------------------\n",
      "Epoch 12 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.2145337164402008\n",
      "Average Validation Loss : 0.4299739599227905\n",
      "--------------------------------------------------\n",
      "Epoch 13 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.20965969562530518\n",
      "Average Validation Loss : 0.39488714933395386\n",
      "--------------------------------------------------\n",
      "Epoch 14 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.20548592507839203\n",
      "Average Validation Loss : 0.36558467149734497\n",
      "--------------------------------------------------\n",
      "Epoch 15 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.20164130628108978\n",
      "Average Validation Loss : 0.34371674060821533\n",
      "--------------------------------------------------\n",
      "Epoch 16 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.19827662408351898\n",
      "Average Validation Loss : 0.3211005926132202\n",
      "--------------------------------------------------\n",
      "Epoch 17 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.19439519941806793\n",
      "Average Validation Loss : 0.300967812538147\n",
      "--------------------------------------------------\n",
      "Epoch 18 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.18986254930496216\n",
      "Average Validation Loss : 0.2888578772544861\n",
      "--------------------------------------------------\n",
      "Epoch 19 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.18709567189216614\n",
      "Average Validation Loss : 0.2706539034843445\n",
      "--------------------------------------------------\n",
      "Epoch 20 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.18058018386363983\n",
      "Average Validation Loss : 0.25408250093460083\n",
      "--------------------------------------------------\n",
      "Epoch 21 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.17818717658519745\n",
      "Average Validation Loss : 0.24468889832496643\n",
      "--------------------------------------------------\n",
      "Epoch 22 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.17502641677856445\n",
      "Average Validation Loss : 0.23272401094436646\n",
      "--------------------------------------------------\n",
      "Epoch 23 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.16729305684566498\n",
      "Average Validation Loss : 0.21953202784061432\n",
      "--------------------------------------------------\n",
      "Epoch 24 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.16473819315433502\n",
      "Average Validation Loss : 0.21424748003482819\n",
      "--------------------------------------------------\n",
      "Epoch 25 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1608380526304245\n",
      "Average Validation Loss : 0.20844660699367523\n",
      "--------------------------------------------------\n",
      "Epoch 26 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1593485176563263\n",
      "Average Validation Loss : 0.20396263897418976\n",
      "--------------------------------------------------\n",
      "Epoch 27 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.15514373779296875\n",
      "Average Validation Loss : 0.18809258937835693\n",
      "--------------------------------------------------\n",
      "Epoch 28 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.15636199712753296\n",
      "Average Validation Loss : 0.19331461191177368\n",
      "--------------------------------------------------\n",
      "Epoch 29 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.15276972949504852\n",
      "Average Validation Loss : 0.1808462142944336\n",
      "--------------------------------------------------\n",
      "Epoch 30 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.15159399807453156\n",
      "Average Validation Loss : 0.18859976530075073\n",
      "--------------------------------------------------\n",
      "Epoch 31 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1490047425031662\n",
      "Average Validation Loss : 0.1730131208896637\n",
      "--------------------------------------------------\n",
      "Epoch 32 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14724910259246826\n",
      "Average Validation Loss : 0.16893000900745392\n",
      "--------------------------------------------------\n",
      "Epoch 33 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14667201042175293\n",
      "Average Validation Loss : 0.16557671129703522\n",
      "--------------------------------------------------\n",
      "Epoch 34 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1456015557050705\n",
      "Average Validation Loss : 0.16837608814239502\n",
      "--------------------------------------------------\n",
      "Epoch 35 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14327436685562134\n",
      "Average Validation Loss : 0.15979653596878052\n",
      "--------------------------------------------------\n",
      "Epoch 36 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14386394619941711\n",
      "Average Validation Loss : 0.16269786655902863\n",
      "--------------------------------------------------\n",
      "Epoch 37 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1406339854001999\n",
      "Average Validation Loss : 0.15436288714408875\n",
      "--------------------------------------------------\n",
      "Epoch 38 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14303529262542725\n",
      "Average Validation Loss : 0.1550697684288025\n",
      "--------------------------------------------------\n",
      "Epoch 39 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1405070275068283\n",
      "Average Validation Loss : 0.15827813744544983\n",
      "--------------------------------------------------\n",
      "Epoch 40 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14153458178043365\n",
      "Average Validation Loss : 0.1509164422750473\n",
      "--------------------------------------------------\n",
      "Epoch 41 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13919222354888916\n",
      "Average Validation Loss : 0.15190763771533966\n",
      "--------------------------------------------------\n",
      "Epoch 42 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14179015159606934\n",
      "Average Validation Loss : 0.15192285180091858\n",
      "--------------------------------------------------\n",
      "Epoch 43 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1401359885931015\n",
      "Average Validation Loss : 0.147531196475029\n",
      "--------------------------------------------------\n",
      "Epoch 44 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13896481692790985\n",
      "Average Validation Loss : 0.14843687415122986\n",
      "--------------------------------------------------\n",
      "Epoch 45 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14012227952480316\n",
      "Average Validation Loss : 0.15038664638996124\n",
      "--------------------------------------------------\n",
      "Epoch 46 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13907559216022491\n",
      "Average Validation Loss : 0.1539643406867981\n",
      "--------------------------------------------------\n",
      "Epoch 47 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13708539307117462\n",
      "Average Validation Loss : 0.14630413055419922\n",
      "--------------------------------------------------\n",
      "Epoch 48 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1381848305463791\n",
      "Average Validation Loss : 0.14132121205329895\n",
      "--------------------------------------------------\n",
      "Epoch 49 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1371639221906662\n",
      "Average Validation Loss : 0.1511143147945404\n",
      "--------------------------------------------------\n",
      "Epoch 50 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13658122718334198\n",
      "Average Validation Loss : 0.14161962270736694\n",
      "--------------------------------------------------\n",
      "Epoch 51 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13896064460277557\n",
      "Average Validation Loss : 0.14175307750701904\n",
      "--------------------------------------------------\n",
      "Epoch 52 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13659019768238068\n",
      "Average Validation Loss : 0.14814987778663635\n",
      "--------------------------------------------------\n",
      "Epoch 53 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1361040323972702\n",
      "Average Validation Loss : 0.14104172587394714\n",
      "--------------------------------------------------\n",
      "Epoch 54 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13609106838703156\n",
      "Average Validation Loss : 0.13971427083015442\n",
      "--------------------------------------------------\n",
      "Epoch 55 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13519740104675293\n",
      "Average Validation Loss : 0.1394682675600052\n",
      "--------------------------------------------------\n",
      "Epoch 56 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13502077758312225\n",
      "Average Validation Loss : 0.13642169535160065\n",
      "--------------------------------------------------\n",
      "Epoch 57 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13531695306301117\n",
      "Average Validation Loss : 0.1362210214138031\n",
      "--------------------------------------------------\n",
      "Epoch 58 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13694937527179718\n",
      "Average Validation Loss : 0.14039315283298492\n",
      "--------------------------------------------------\n",
      "Epoch 59 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13567647337913513\n",
      "Average Validation Loss : 0.13864925503730774\n",
      "--------------------------------------------------\n",
      "Epoch 60 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1349896490573883\n",
      "Average Validation Loss : 0.13866344094276428\n",
      "--------------------------------------------------\n",
      "Epoch 61 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13459740579128265\n",
      "Average Validation Loss : 0.13790525496006012\n",
      "--------------------------------------------------\n",
      "Epoch 62 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13477723300457\n",
      "Average Validation Loss : 0.13668741285800934\n",
      "--------------------------------------------------\n",
      "Epoch 63 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13453833758831024\n",
      "Average Validation Loss : 0.13617980480194092\n",
      "--------------------------------------------------\n",
      "Epoch 64 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13430705666542053\n",
      "Average Validation Loss : 0.13570883870124817\n",
      "--------------------------------------------------\n",
      "Epoch 65 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13483355939388275\n",
      "Average Validation Loss : 0.1394692063331604\n",
      "--------------------------------------------------\n",
      "Epoch 66 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13508668541908264\n",
      "Average Validation Loss : 0.1377589851617813\n",
      "--------------------------------------------------\n",
      "Epoch 67 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13372725248336792\n",
      "Average Validation Loss : 0.13399872183799744\n",
      "--------------------------------------------------\n",
      "Epoch 68 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13249260187149048\n",
      "Average Validation Loss : 0.14037644863128662\n",
      "--------------------------------------------------\n",
      "Epoch 69 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13437628746032715\n",
      "Average Validation Loss : 0.13540765643119812\n",
      "--------------------------------------------------\n",
      "Epoch 70 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1331852674484253\n",
      "Average Validation Loss : 0.13598290085792542\n",
      "--------------------------------------------------\n",
      "Epoch 71 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13348738849163055\n",
      "Average Validation Loss : 0.13726606965065002\n",
      "--------------------------------------------------\n",
      "Epoch 72 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13442127406597137\n",
      "Average Validation Loss : 0.13635927438735962\n",
      "--------------------------------------------------\n",
      "Epoch 73 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13347099721431732\n",
      "Average Validation Loss : 0.14009889960289001\n",
      "--------------------------------------------------\n",
      "Epoch 74 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1334313154220581\n",
      "Average Validation Loss : 0.135747492313385\n",
      "--------------------------------------------------\n",
      "Epoch 75 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13261960446834564\n",
      "Average Validation Loss : 0.1331859827041626\n",
      "--------------------------------------------------\n",
      "Epoch 76 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13135474920272827\n",
      "Average Validation Loss : 0.13677698373794556\n",
      "--------------------------------------------------\n",
      "Epoch 77 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1329789012670517\n",
      "Average Validation Loss : 0.14144539833068848\n",
      "--------------------------------------------------\n",
      "Epoch 78 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13462024927139282\n",
      "Average Validation Loss : 0.1471613049507141\n",
      "--------------------------------------------------\n",
      "Epoch 79 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1341109275817871\n",
      "Average Validation Loss : 0.13593529164791107\n",
      "--------------------------------------------------\n",
      "Epoch 80 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1331690549850464\n",
      "Average Validation Loss : 0.13330572843551636\n",
      "--------------------------------------------------\n",
      "Epoch 81 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13278281688690186\n",
      "Average Validation Loss : 0.1385754644870758\n",
      "--------------------------------------------------\n",
      "Epoch 82 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13430415093898773\n",
      "Average Validation Loss : 0.13460960984230042\n",
      "--------------------------------------------------\n",
      "Epoch 83 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13170158863067627\n",
      "Average Validation Loss : 0.1347285956144333\n",
      "--------------------------------------------------\n",
      "Epoch 84 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1315046101808548\n",
      "Average Validation Loss : 0.13430695235729218\n",
      "--------------------------------------------------\n",
      "Epoch 85 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13269160687923431\n",
      "Average Validation Loss : 0.13758906722068787\n",
      "--------------------------------------------------\n",
      "Epoch 86 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13072200119495392\n",
      "Average Validation Loss : 0.13353610038757324\n",
      "--------------------------------------------------\n",
      "Epoch 87 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13199494779109955\n",
      "Average Validation Loss : 0.13421663641929626\n",
      "--------------------------------------------------\n",
      "Epoch 88 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13081906735897064\n",
      "Average Validation Loss : 0.13179486989974976\n",
      "--------------------------------------------------\n",
      "Epoch 89 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1313556283712387\n",
      "Average Validation Loss : 0.13754993677139282\n",
      "--------------------------------------------------\n",
      "Epoch 90 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13489454984664917\n",
      "Average Validation Loss : 0.13779771327972412\n",
      "--------------------------------------------------\n",
      "Epoch 91 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1325550377368927\n",
      "Average Validation Loss : 0.13130204379558563\n",
      "--------------------------------------------------\n",
      "Epoch 92 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13195252418518066\n",
      "Average Validation Loss : 0.13393831253051758\n",
      "--------------------------------------------------\n",
      "Epoch 93 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1303999274969101\n",
      "Average Validation Loss : 0.1355128437280655\n",
      "--------------------------------------------------\n",
      "Epoch 94 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13151828944683075\n",
      "Average Validation Loss : 0.13408079743385315\n",
      "--------------------------------------------------\n",
      "Epoch 95 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13038025796413422\n",
      "Average Validation Loss : 0.13214464485645294\n",
      "--------------------------------------------------\n",
      "Epoch 96 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13056927919387817\n",
      "Average Validation Loss : 0.13622666895389557\n",
      "--------------------------------------------------\n",
      "Epoch 97 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13030290603637695\n",
      "Average Validation Loss : 0.1313779652118683\n",
      "--------------------------------------------------\n",
      "Epoch 98 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13028517365455627\n",
      "Average Validation Loss : 0.1344546377658844\n",
      "--------------------------------------------------\n",
      "Epoch 99 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.1323263794183731\n",
      "Average Validation Loss : 0.13303875923156738\n",
      "--------------------------------------------------\n",
      "Epoch 100 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.13047242164611816\n",
      "Average Validation Loss : 0.13483405113220215\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_AEI(\n",
    "    model, num_freq,\n",
    "    train_loader, valid_loader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate)\n",
    "\n",
    "to.save(model.state_dict(), 'SAE_model.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17.0052,  3.4707, 14.4204, 14.6054, 19.6811], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data\n",
    "data = pd.read_csv('Data Workspace\\FM_Te_Data.csv')\n",
    "scaled_data = data[X_names+Y_names].copy()\n",
    "\n",
    "scaled_data[freq_name(num_freq,1,0)] = np.log(scaled_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scaled_data[X_names]= scalerX.transform(scaled_data[X_names])\n",
    "scaled_data[Y_names]= scalerY.transform(scaled_data[Y_names])\n",
    "\n",
    "scaled_data = SAE_data(scaled_data, X_names, Y_names)\n",
    "test_loader = to.utils.data.DataLoader(scaled_data, batch_size=len(scaled_data), shuffle=False)\n",
    "\n",
    "with to.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        X, Y = data\n",
    "\n",
    "        N = to.randn((X.shape[0], num_Z), device=device)\n",
    "        predictY = model.decode(X, N)\n",
    "\n",
    "        Y = to.from_numpy(scalerY.inverse_transform(Y.to('cpu'))).to(device)\n",
    "        predictY = to.from_numpy(scalerY.inverse_transform(predictY.to('cpu'))).to(device)\n",
    "            \n",
    "        abs_perc_error = to.abs((Y- predictY)/Y)*100\n",
    "        MAPE_per_dim = to.mean(abs_perc_error, 0)\n",
    "\n",
    "print(MAPE_per_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6331,  0.4060,  2.4802,  2.2927,  2.2828],\n",
       "        [14.6085,  1.3770,  9.8466,  7.7057,  6.4808],\n",
       "        [ 7.0710,  0.6241,  7.8435,  7.5034,  7.4043],\n",
       "        ...,\n",
       "        [10.0746,  0.9101, 21.8129, 20.6953, 54.7179],\n",
       "        [24.4656,  0.2298, 12.5875, 11.3198,  7.6981],\n",
       "        [ 1.9595, 18.0342,  6.6779,  7.9423, 10.9633]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_perc_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 4\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(Y[:,num].to('cpu'), predictY[:,num].to('cpu'))\n",
    "plt.xlim(0,max(1.2*max(Y[:,num].to('cpu')),1.2*max(Y[:,num].to('cpu'))))\n",
    "plt.ylim(0,max(1.2*max(Y[:,num].to('cpu')),1.2*max(Y[:,num].to('cpu'))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2426e189533ac41f5aad4fbc5dc1b573401ba6ae3fed99d7f2363912ea400c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
