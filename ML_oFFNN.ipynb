{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import torch as to\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import torch.utils.data as to_data\n",
    "from torch.utils.tensorboard import SummaryWriter as sumwriter\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Specify hardware for ML training (GPU default)\n",
    "device = \"cuda\" if to.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly generate list of strings for frequency numbers and ratios\n",
    "def freq_name(no_freq, include_freq=True, include_ratio=True):\n",
    "    \"\"\"\n",
    "    Creates an ordered list of string from inputted parameters:\n",
    "\n",
    "    no_freq = (int) number of desired frequencies\n",
    "    include_freq = (bool) include the individual frequencies or not (default True)\n",
    "    include_ratio = (bool) include the non-trivial ratios between frequencies or not (default True)\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    if include_freq:\n",
    "        for i in range(no_freq):\n",
    "            names.append('f'+str(i+1))\n",
    "    if include_ratio:\n",
    "        for i in range(no_freq):\n",
    "            for j in range(i):\n",
    "                names.append('f'+str(i+1)+'/f'+str(j+1))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pytorch dataset class for data batching during training\n",
    "class FFNN_data(to_data.Dataset):\n",
    "    def __init__(self, scaled_dataframe, X_names, Y_names):\n",
    "        self.len = len(scaled_dataframe)\n",
    "        self.numfreqs = len(X_names)\n",
    "        self.X = to.from_numpy(scaled_dataframe[X_names].to_numpy().astype('float32')).to(device)\n",
    "        self.Y = to.from_numpy(scaled_dataframe[Y_names].to_numpy().astype('float32')).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        shuffl_inds = to.randperm(self.numfreqs, device=device)\n",
    "        X_idx = self.X[idx,:][shuffl_inds]\n",
    "        # mode_nums = to.flatten(to.tril(to.ones((self.numfreqs,self.numfreqs), device=device))[shuffl_inds,:])\n",
    "        # X_idx = to.cat((mode_nums, X_idx))\n",
    "        X_idx = to.cat((shuffl_inds, X_idx))\n",
    "        Y_idx = self.Y[idx,:]\n",
    "        return X_idx, Y_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates function that returns desired activation function\n",
    "def activation(activ_name):\n",
    "    if activ_name=='relu':\n",
    "        return to.nn.ReLU()\n",
    "    elif activ_name=='lrelu':\n",
    "        return to.nn.LeakyReLU()\n",
    "    elif activ_name=='prelu':\n",
    "        return to.nn.PReLU()\n",
    "    elif activ_name=='relu6':\n",
    "        return to.nn.ReLU6()\n",
    "    elif activ_name=='sigmoid':\n",
    "        return to.nn.Sigmoid()\n",
    "    elif activ_name=='tanh':\n",
    "        return to.nn.Tanh()\n",
    "    elif activ_name=='silu':\n",
    "        return to.nn.SiLU()\n",
    "    elif activ_name=='selu':\n",
    "        return to.nn.SELU()\n",
    "    elif activ_name=='celu':\n",
    "        return to.nn.CELU()\n",
    "    elif activ_name=='gelu':\n",
    "        return to.nn.GELU()\n",
    "    else:\n",
    "        return to.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF_Network(to.nn.Module):\n",
    "    def __init__(self, num_X, num_Y, h_nodes, hactiv_type):\n",
    "        super(FF_Network, self).__init__()\n",
    "\n",
    "        self.feedfoward = []\n",
    "        self.feedfoward.append(to.nn.Linear(num_X, h_nodes[0]))\n",
    "        self.feedfoward.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(h_nodes)-1):\n",
    "            self.feedfoward.append(to.nn.Linear(h_nodes[i], h_nodes[i+1]))\n",
    "            self.feedfoward.append(activation(hactiv_type))\n",
    "\n",
    "        self.feedfoward.append(to.nn.Linear(h_nodes[-1], num_Y))\n",
    "\n",
    "        self.feedfoward = to.nn.Sequential(*self.feedfoward).to(device)\n",
    "        for i in self.feedfoward[::2]:\n",
    "            to.nn.init.kaiming_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = self.feedfoward(X)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network,\n",
    "    train_dataloader,\n",
    "    loss_function, optimizer,\n",
    "    tb_writer, epoch_ind\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "    MAPE_list = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        X, Y = data\n",
    "\n",
    "        if epoch_ind==0 and i==0:\n",
    "            tb_writer.add_graph(network, X, verbose=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictY = network(X)\n",
    "\n",
    "        loss = loss_function(predictY, Y)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        MAPE = to.mean(to.abs((Y - predictY) / Y)*100)\n",
    "        MAPE_list.append(MAPE.item())\n",
    "\n",
    "    \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "    mean_MAPE = to.mean(to.tensor(MAPE_list, device=device)).item()\n",
    "\n",
    "    return mean_loss, mean_MAPE\n",
    "\n",
    "def valid_epoch(\n",
    "    network,\n",
    "    valid_dataloader,\n",
    "    loss_function\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "    MAPE_list = []\n",
    "\n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        X, Y = data\n",
    "        predictY = network(X)\n",
    "\n",
    "        loss = loss_function(predictY, Y)\n",
    "        loss_list.append(loss.item())\n",
    " \n",
    "        MAPE = to.mean(to.abs((Y - predictY) / Y)*100)\n",
    "        MAPE_list.append(MAPE.item())\n",
    "    \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "    mean_MAPE = to.mean(to.tensor(MAPE_list, device=device)).item()\n",
    "    return mean_loss, mean_MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AEI(\n",
    "    network,\n",
    "    train_dataloader, valid_dataloader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate\n",
    "    ):\n",
    "\n",
    "    if optimizer_type=='adam':\n",
    "        optimizer = to.optim.Adam(network.parameters(), lr=learn_rate)\n",
    "    else:\n",
    "        optimizer = to.optim.SGD(network.parameters(), lr=learn_rate)\n",
    "    \n",
    "    tb_writer = sumwriter('Current_ML_Results/Tensorboard')\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        network.train(True)\n",
    "        mloss, mMAPE = train_epoch(network, train_dataloader, loss_function, optimizer, tb_writer, i)\n",
    "\n",
    "        network.eval()\n",
    "        with to.no_grad():\n",
    "            vmloss, vmMAPE = valid_epoch(network, valid_dataloader, loss_function)\n",
    "        \n",
    "\n",
    "        print('-'*50)\n",
    "        print('Epoch {} / {}'.format(i+1,epochs))\n",
    "        print('-'*15)\n",
    "        print('Average Train Loss : {}'.format(mloss))\n",
    "        print('Average Validation Loss : {}'.format(vmloss))\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch Mean Loss\",\n",
    "                            {\n",
    "                                'Train' : mloss,\n",
    "                                'Validation' : vmloss\n",
    "                            }, i+1)\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch MAPE\",\n",
    "                            {\n",
    "                                'Train' : mMAPE,\n",
    "                                'Validation' : vmMAPE\n",
    "                            }, i+1)\n",
    "\n",
    "    tb_writer.flush()\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "data = pd.read_csv('Data Workspace/FM_TV_Data.csv')\n",
    "num_freq = 5\n",
    "\n",
    "features = freq_name(num_freq,1,0)\n",
    "labels = ['psi', 'nu', 'a', 'b']\n",
    "\n",
    "train_split = int(0.8*len(data))\n",
    "valid_split = len(data)- train_split\n",
    "\n",
    "scaled_data = data[features+labels].copy()\n",
    "scaled_data[freq_name(num_freq,1,0)] = np.log(scaled_data[freq_name(num_freq,1,0)])\n",
    "scaled_data['psi'] = np.log(scaled_data['psi'])\n",
    "# scaled_data['t'] = scaled_data['t']*100\n",
    "# scaled_data['E'] = scaled_data['E']/1e11\n",
    "# scaled_data['rho'] = scaled_data['rho']/10000\n",
    "\n",
    "scaled_data = FFNN_data(scaled_data, features, labels)\n",
    "train_set, valid_set = to_data.random_split(scaled_data, [train_split, valid_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "# Parameters\n",
    "num_X = len(features)+len(features)\n",
    "num_Y = len(labels)\n",
    "h_nodes = [20,30,40,50,60,50,40,30,20,10,6]\n",
    "hactiv = 'prelu'\n",
    "\n",
    "batch_size_train = 2000\n",
    "batch_size_valid = 2000\n",
    "\n",
    "epochs = 100\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# Optim Selections\n",
    "loss_function = to.nn.SmoothL1Loss()\n",
    "optimizer_type = 'adam'\n",
    "\n",
    "# Data loaders\n",
    "train_loader = to.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True)\n",
    "valid_loader = to.utils.data.DataLoader(valid_set, batch_size=batch_size_valid, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = FF_Network(num_X, num_Y, h_nodes, hactiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Epoch 1 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.8244194984436035\n",
      "Average Validation Loss : 0.5230622887611389\n",
      "--------------------------------------------------\n",
      "Epoch 2 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.47519874572753906\n",
      "Average Validation Loss : 0.4286484718322754\n",
      "--------------------------------------------------\n",
      "Epoch 3 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.34353911876678467\n",
      "Average Validation Loss : 0.22955931723117828\n",
      "--------------------------------------------------\n",
      "Epoch 4 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.14830079674720764\n",
      "Average Validation Loss : 0.10247163474559784\n",
      "--------------------------------------------------\n",
      "Epoch 5 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.09865142405033112\n",
      "Average Validation Loss : 0.09563860297203064\n",
      "--------------------------------------------------\n",
      "Epoch 6 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.09334126114845276\n",
      "Average Validation Loss : 0.09480893611907959\n",
      "--------------------------------------------------\n",
      "Epoch 7 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.09106616675853729\n",
      "Average Validation Loss : 0.09168136864900589\n",
      "--------------------------------------------------\n",
      "Epoch 8 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08949431777000427\n",
      "Average Validation Loss : 0.09038865566253662\n",
      "--------------------------------------------------\n",
      "Epoch 9 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08848841488361359\n",
      "Average Validation Loss : 0.0888550728559494\n",
      "--------------------------------------------------\n",
      "Epoch 10 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08801720291376114\n",
      "Average Validation Loss : 0.0881117507815361\n",
      "--------------------------------------------------\n",
      "Epoch 11 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08769600093364716\n",
      "Average Validation Loss : 0.09088180959224701\n",
      "--------------------------------------------------\n",
      "Epoch 12 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08819803595542908\n",
      "Average Validation Loss : 0.08777748048305511\n",
      "--------------------------------------------------\n",
      "Epoch 13 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08709993958473206\n",
      "Average Validation Loss : 0.08772386610507965\n",
      "--------------------------------------------------\n",
      "Epoch 14 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08732452988624573\n",
      "Average Validation Loss : 0.08697983622550964\n",
      "--------------------------------------------------\n",
      "Epoch 15 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08547411859035492\n",
      "Average Validation Loss : 0.08706284314393997\n",
      "--------------------------------------------------\n",
      "Epoch 16 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0862443596124649\n",
      "Average Validation Loss : 0.08636729419231415\n",
      "--------------------------------------------------\n",
      "Epoch 17 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08434230834245682\n",
      "Average Validation Loss : 0.08727417886257172\n",
      "--------------------------------------------------\n",
      "Epoch 18 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0857088640332222\n",
      "Average Validation Loss : 0.08614268898963928\n",
      "--------------------------------------------------\n",
      "Epoch 19 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08684779703617096\n",
      "Average Validation Loss : 0.08482532203197479\n",
      "--------------------------------------------------\n",
      "Epoch 20 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08373146504163742\n",
      "Average Validation Loss : 0.084990493953228\n",
      "--------------------------------------------------\n",
      "Epoch 21 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0837552547454834\n",
      "Average Validation Loss : 0.08435320854187012\n",
      "--------------------------------------------------\n",
      "Epoch 22 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08301743119955063\n",
      "Average Validation Loss : 0.08601630479097366\n",
      "--------------------------------------------------\n",
      "Epoch 23 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08282362669706345\n",
      "Average Validation Loss : 0.08361451327800751\n",
      "--------------------------------------------------\n",
      "Epoch 24 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08300158381462097\n",
      "Average Validation Loss : 0.08829943090677261\n",
      "--------------------------------------------------\n",
      "Epoch 25 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08472909033298492\n",
      "Average Validation Loss : 0.08540821075439453\n",
      "--------------------------------------------------\n",
      "Epoch 26 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08221553266048431\n",
      "Average Validation Loss : 0.08371321856975555\n",
      "--------------------------------------------------\n",
      "Epoch 27 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0817490667104721\n",
      "Average Validation Loss : 0.08232256770133972\n",
      "--------------------------------------------------\n",
      "Epoch 28 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08215301483869553\n",
      "Average Validation Loss : 0.08587317913770676\n",
      "--------------------------------------------------\n",
      "Epoch 29 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08268724381923676\n",
      "Average Validation Loss : 0.0836874470114708\n",
      "--------------------------------------------------\n",
      "Epoch 30 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08287537097930908\n",
      "Average Validation Loss : 0.08290721476078033\n",
      "--------------------------------------------------\n",
      "Epoch 31 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08103000372648239\n",
      "Average Validation Loss : 0.08265574276447296\n",
      "--------------------------------------------------\n",
      "Epoch 32 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08175338059663773\n",
      "Average Validation Loss : 0.0837896466255188\n",
      "--------------------------------------------------\n",
      "Epoch 33 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08245278894901276\n",
      "Average Validation Loss : 0.08374394476413727\n",
      "--------------------------------------------------\n",
      "Epoch 34 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08075162768363953\n",
      "Average Validation Loss : 0.08264510333538055\n",
      "--------------------------------------------------\n",
      "Epoch 35 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08112826943397522\n",
      "Average Validation Loss : 0.08179644495248795\n",
      "--------------------------------------------------\n",
      "Epoch 36 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08004479110240936\n",
      "Average Validation Loss : 0.08125822246074677\n",
      "--------------------------------------------------\n",
      "Epoch 37 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07985452562570572\n",
      "Average Validation Loss : 0.0826994776725769\n",
      "--------------------------------------------------\n",
      "Epoch 38 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07968448102474213\n",
      "Average Validation Loss : 0.0816078707575798\n",
      "--------------------------------------------------\n",
      "Epoch 39 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07987823337316513\n",
      "Average Validation Loss : 0.08059584349393845\n",
      "--------------------------------------------------\n",
      "Epoch 40 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.08055059611797333\n",
      "Average Validation Loss : 0.08065645396709442\n",
      "--------------------------------------------------\n",
      "Epoch 41 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07872859388589859\n",
      "Average Validation Loss : 0.08026581257581711\n",
      "--------------------------------------------------\n",
      "Epoch 42 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07928424328565598\n",
      "Average Validation Loss : 0.08069516718387604\n",
      "--------------------------------------------------\n",
      "Epoch 43 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07930460572242737\n",
      "Average Validation Loss : 0.0800573080778122\n",
      "--------------------------------------------------\n",
      "Epoch 44 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0781867653131485\n",
      "Average Validation Loss : 0.08017444610595703\n",
      "--------------------------------------------------\n",
      "Epoch 45 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07829966396093369\n",
      "Average Validation Loss : 0.07996320724487305\n",
      "--------------------------------------------------\n",
      "Epoch 46 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07825997471809387\n",
      "Average Validation Loss : 0.07999013364315033\n",
      "--------------------------------------------------\n",
      "Epoch 47 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07948804646730423\n",
      "Average Validation Loss : 0.07932280004024506\n",
      "--------------------------------------------------\n",
      "Epoch 48 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0780763328075409\n",
      "Average Validation Loss : 0.07922352105379105\n",
      "--------------------------------------------------\n",
      "Epoch 49 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07889948040246964\n",
      "Average Validation Loss : 0.07898467779159546\n",
      "--------------------------------------------------\n",
      "Epoch 50 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07882480323314667\n",
      "Average Validation Loss : 0.08062978088855743\n",
      "--------------------------------------------------\n",
      "Epoch 51 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07815499603748322\n",
      "Average Validation Loss : 0.0788787305355072\n",
      "--------------------------------------------------\n",
      "Epoch 52 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07771811634302139\n",
      "Average Validation Loss : 0.07953190058469772\n",
      "--------------------------------------------------\n",
      "Epoch 53 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07812055945396423\n",
      "Average Validation Loss : 0.07884863018989563\n",
      "--------------------------------------------------\n",
      "Epoch 54 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07769078016281128\n",
      "Average Validation Loss : 0.07986587285995483\n",
      "--------------------------------------------------\n",
      "Epoch 55 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0772344097495079\n",
      "Average Validation Loss : 0.0783831924200058\n",
      "--------------------------------------------------\n",
      "Epoch 56 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07719972729682922\n",
      "Average Validation Loss : 0.07863939553499222\n",
      "--------------------------------------------------\n",
      "Epoch 57 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07679003477096558\n",
      "Average Validation Loss : 0.07831481099128723\n",
      "--------------------------------------------------\n",
      "Epoch 58 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07756449282169342\n",
      "Average Validation Loss : 0.07912653684616089\n",
      "--------------------------------------------------\n",
      "Epoch 59 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07700137794017792\n",
      "Average Validation Loss : 0.08014451712369919\n",
      "--------------------------------------------------\n",
      "Epoch 60 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07777823507785797\n",
      "Average Validation Loss : 0.07790572196245193\n",
      "--------------------------------------------------\n",
      "Epoch 61 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07949602603912354\n",
      "Average Validation Loss : 0.08097362518310547\n",
      "--------------------------------------------------\n",
      "Epoch 62 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07822292298078537\n",
      "Average Validation Loss : 0.07818871736526489\n",
      "--------------------------------------------------\n",
      "Epoch 63 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07735280692577362\n",
      "Average Validation Loss : 0.07850007712841034\n",
      "--------------------------------------------------\n",
      "Epoch 64 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07654717564582825\n",
      "Average Validation Loss : 0.07766887545585632\n",
      "--------------------------------------------------\n",
      "Epoch 65 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07669581472873688\n",
      "Average Validation Loss : 0.07744772732257843\n",
      "--------------------------------------------------\n",
      "Epoch 66 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07596780359745026\n",
      "Average Validation Loss : 0.07721589505672455\n",
      "--------------------------------------------------\n",
      "Epoch 67 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0762493759393692\n",
      "Average Validation Loss : 0.07796158641576767\n",
      "--------------------------------------------------\n",
      "Epoch 68 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07713481783866882\n",
      "Average Validation Loss : 0.07850593328475952\n",
      "--------------------------------------------------\n",
      "Epoch 69 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07647629082202911\n",
      "Average Validation Loss : 0.0773584321141243\n",
      "--------------------------------------------------\n",
      "Epoch 70 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07575231790542603\n",
      "Average Validation Loss : 0.07790409028530121\n",
      "--------------------------------------------------\n",
      "Epoch 71 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07576719671487808\n",
      "Average Validation Loss : 0.0767495334148407\n",
      "--------------------------------------------------\n",
      "Epoch 72 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07528285682201385\n",
      "Average Validation Loss : 0.07743556797504425\n",
      "--------------------------------------------------\n",
      "Epoch 73 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07610422372817993\n",
      "Average Validation Loss : 0.07702634483575821\n",
      "--------------------------------------------------\n",
      "Epoch 74 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07616997510194778\n",
      "Average Validation Loss : 0.07735738158226013\n",
      "--------------------------------------------------\n",
      "Epoch 75 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07553774863481522\n",
      "Average Validation Loss : 0.07720530033111572\n",
      "--------------------------------------------------\n",
      "Epoch 76 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07531972974538803\n",
      "Average Validation Loss : 0.0765736922621727\n",
      "--------------------------------------------------\n",
      "Epoch 77 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07502827048301697\n",
      "Average Validation Loss : 0.07664252072572708\n",
      "--------------------------------------------------\n",
      "Epoch 78 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07546237111091614\n",
      "Average Validation Loss : 0.0768926665186882\n",
      "--------------------------------------------------\n",
      "Epoch 79 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07520607113838196\n",
      "Average Validation Loss : 0.07642897963523865\n",
      "--------------------------------------------------\n",
      "Epoch 80 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0751681923866272\n",
      "Average Validation Loss : 0.07818695902824402\n",
      "--------------------------------------------------\n",
      "Epoch 81 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07609690725803375\n",
      "Average Validation Loss : 0.07628953456878662\n",
      "--------------------------------------------------\n",
      "Epoch 82 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07556939125061035\n",
      "Average Validation Loss : 0.07646791636943817\n",
      "--------------------------------------------------\n",
      "Epoch 83 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07538148760795593\n",
      "Average Validation Loss : 0.07589809596538544\n",
      "--------------------------------------------------\n",
      "Epoch 84 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07460525631904602\n",
      "Average Validation Loss : 0.0759505033493042\n",
      "--------------------------------------------------\n",
      "Epoch 85 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07414796948432922\n",
      "Average Validation Loss : 0.0757850781083107\n",
      "--------------------------------------------------\n",
      "Epoch 86 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0744275152683258\n",
      "Average Validation Loss : 0.07588442414999008\n",
      "--------------------------------------------------\n",
      "Epoch 87 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07605686783790588\n",
      "Average Validation Loss : 0.07842554152011871\n",
      "--------------------------------------------------\n",
      "Epoch 88 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07525509595870972\n",
      "Average Validation Loss : 0.07553031295537949\n",
      "--------------------------------------------------\n",
      "Epoch 89 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.0745556429028511\n",
      "Average Validation Loss : 0.07618562132120132\n",
      "--------------------------------------------------\n",
      "Epoch 90 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07539878785610199\n",
      "Average Validation Loss : 0.07690776884555817\n",
      "--------------------------------------------------\n",
      "Epoch 91 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07668928802013397\n",
      "Average Validation Loss : 0.0757143646478653\n",
      "--------------------------------------------------\n",
      "Epoch 92 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07442544400691986\n",
      "Average Validation Loss : 0.07590006291866302\n",
      "--------------------------------------------------\n",
      "Epoch 93 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07412439584732056\n",
      "Average Validation Loss : 0.0757949948310852\n",
      "--------------------------------------------------\n",
      "Epoch 94 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07470053434371948\n",
      "Average Validation Loss : 0.07622133195400238\n",
      "--------------------------------------------------\n",
      "Epoch 95 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07420738786458969\n",
      "Average Validation Loss : 0.07522672414779663\n",
      "--------------------------------------------------\n",
      "Epoch 96 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07454610615968704\n",
      "Average Validation Loss : 0.07724637538194656\n",
      "--------------------------------------------------\n",
      "Epoch 97 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07437428086996078\n",
      "Average Validation Loss : 0.07676349580287933\n",
      "--------------------------------------------------\n",
      "Epoch 98 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07421638816595078\n",
      "Average Validation Loss : 0.07492763549089432\n",
      "--------------------------------------------------\n",
      "Epoch 99 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07405100762844086\n",
      "Average Validation Loss : 0.0758119598031044\n",
      "--------------------------------------------------\n",
      "Epoch 100 / 100\n",
      "---------------\n",
      "Average Train Loss : 0.07438181340694427\n",
      "Average Validation Loss : 0.0760577917098999\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_AEI(\n",
    "    model,\n",
    "    train_loader, valid_loader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate)\n",
    "\n",
    "to.save(model.state_dict(), 'Current_ML_Results/model.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Errors: \n",
      "--------------------\n",
      "f1: 72.90 %\n",
      "f2: 16.93 %\n",
      "f3: 49.19 %\n",
      "f4: 48.67 %\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(to.load('Current_ML_Results/model.state'))\n",
    "\n",
    "trainvalid_data = pd.read_csv('Data Workspace/FM_TV_Data.csv')\n",
    "scaled_trainvalid_data = trainvalid_data[features+labels].copy()\n",
    "\n",
    "scaled_trainvalid_data['psi'] = np.log(scaled_trainvalid_data['psi'])\n",
    "# scaled_trainvalid_data['E'] = scaled_trainvalid_data['E']/1e11\n",
    "# scaled_trainvalid_data['rho'] = scaled_trainvalid_data['rho']/10000\n",
    "# scaled_trainvalid_data['t'] = scaled_trainvalid_data['t']*100\n",
    "scaled_trainvalid_data[freq_name(num_freq,1,0)] = np.log(scaled_trainvalid_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scaled_trainvalid_data = FFNN_data(scaled_trainvalid_data, features, labels)\n",
    "test_loader = to.utils.data.DataLoader(scaled_trainvalid_data, batch_size=len(scaled_trainvalid_data), shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with to.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        X, Y = data\n",
    "        predictY = model.feedfoward(X)\n",
    "            \n",
    "        Y[:,0] = to.exp(Y[:,0])\n",
    "        predictY[:,0] = to.exp(predictY[:,0])\n",
    "        abs_perc_error = to.abs((Y- predictY)/Y)*100\n",
    "        MAPE_per_dim = to.mean(abs_perc_error, 0)\n",
    "\n",
    "        np.savetxt('Current_ML_Results/MAPE_trainvalid.txt', MAPE_per_dim.cpu().numpy())\n",
    "\n",
    "        print('Absolute Percentage Errors: ')\n",
    "        print('-'*20)\n",
    "        print('f1: {:0.2f} %'.format(MAPE_per_dim[0].item()))\n",
    "        print('f2: {:0.2f} %'.format(MAPE_per_dim[1].item()))\n",
    "        print('f3: {:0.2f} %'.format(MAPE_per_dim[2].item()))\n",
    "        print('f4: {:0.2f} %'.format(MAPE_per_dim[3].item()))\n",
    "        # print('f5: {:0.2f} %'.format(MAPE_per_dim[4].item()))\n",
    "        # print('f6: {:0.2f} %'.format(MAPE_per_dim[5].item()))\n",
    "        # print('f7: {:0.2f} %'.format(MAPE_per_dim[6].item()))\n",
    "        # print('f8: {:0.2f} %'.format(MAPE_per_dim[7].item()))\n",
    "        # print('f9: {:0.2f} %'.format(MAPE_per_dim[8].item()))\n",
    "        # print('f10: {:0.2f} %'.format(MAPE_per_dim[9].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Errors: \n",
      "--------------------\n",
      "f1: 73.86 %\n",
      "f2: 17.06 %\n",
      "f3: 48.86 %\n",
      "f4: 49.30 %\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(to.load('Current_ML_Results/model.state'))\n",
    "\n",
    "test_data = pd.read_csv('Data Workspace/FM_Te_Data.csv')\n",
    "scaled_test_data = test_data[features+labels].copy()\n",
    "\n",
    "scaled_test_data['psi'] = np.log(scaled_test_data['psi'])\n",
    "scaled_test_data[freq_name(num_freq,1,0)] = np.log(scaled_test_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scaled_test_data = FFNN_data(scaled_test_data, features, labels)\n",
    "test_loader = to.utils.data.DataLoader(scaled_test_data, batch_size=len(scaled_test_data), shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with to.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        X, Y = data\n",
    "        predictY = model.feedfoward(X)\n",
    "            \n",
    "        Y[:,0] = to.exp(Y[:,0])\n",
    "        predictY[:,0] = to.exp(predictY[:,0])\n",
    "        abs_perc_error = to.abs((Y- predictY)/Y)*100\n",
    "        MAPE_per_dim = to.mean(abs_perc_error, 0)\n",
    "\n",
    "        np.savetxt('Current_ML_Results/MAPE_test.txt', MAPE_per_dim.cpu().numpy())\n",
    "\n",
    "        print('Absolute Percentage Errors: ')\n",
    "        print('-'*20)\n",
    "        print('f1: {:0.2f} %'.format(MAPE_per_dim[0].item()))\n",
    "        print('f2: {:0.2f} %'.format(MAPE_per_dim[1].item()))\n",
    "        print('f3: {:0.2f} %'.format(MAPE_per_dim[2].item()))\n",
    "        print('f4: {:0.2f} %'.format(MAPE_per_dim[3].item()))\n",
    "        # print('f5: {:0.2f} %'.format(MAPE_per_dim[4].item()))\n",
    "        # print('f6: {:0.2f} %'.format(MAPE_per_dim[5].item()))\n",
    "        # print('f7: {:0.2f} %'.format(MAPE_per_dim[6].item()))\n",
    "        # print('f8: {:0.2f} %'.format(MAPE_per_dim[7].item()))\n",
    "        # print('f9: {:0.2f} %'.format(MAPE_per_dim[8].item()))\n",
    "        # print('f10: {:0.2f} %'.format(MAPE_per_dim[9].item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2426e189533ac41f5aad4fbc5dc1b573401ba6ae3fed99d7f2363912ea400c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
