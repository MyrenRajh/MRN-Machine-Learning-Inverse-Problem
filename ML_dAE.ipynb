{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import torch as to\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import torch.utils.data as to_data\n",
    "from torch.utils.tensorboard import SummaryWriter as sumwriter\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Specify hardware for ML training (GPU default)\n",
    "device = \"cuda\" if to.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly generate list of strings for frequency numbers and ratios\n",
    "def freq_name(no_freq, include_freq=True, include_ratio=True):\n",
    "    \"\"\"\n",
    "    Creates an ordered list of string from inputted parameters:\n",
    "\n",
    "    no_freq = (int) number of desired frequencies\n",
    "    include_freq = (bool) include the individual frequencies or not (default True)\n",
    "    include_ratio = (bool) include the non-trivial ratios between frequencies or not (default True)\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    if include_freq:\n",
    "        for i in range(no_freq):\n",
    "            names.append('f'+str(i+1))\n",
    "    if include_ratio:\n",
    "        for i in range(no_freq):\n",
    "            for j in range(i):\n",
    "                names.append('f'+str(i+1)+'/f'+str(j+1))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pytorch dataset class for data batching during training\n",
    "class FFNN_data(to_data.Dataset):\n",
    "    def __init__(self, scaled_dataframe, X_names, Y_names):\n",
    "        self.len = len(scaled_dataframe)\n",
    "        self.X = to.from_numpy(scaled_dataframe[X_names].to_numpy().astype('float32')).to(device)\n",
    "        self.Y = to.from_numpy(scaled_dataframe[Y_names].to_numpy().astype('float32')).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        X_idx = self.X[idx,:]\n",
    "        Y_idx = self.Y[idx,:]\n",
    "        return X_idx, Y_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates function that returns desired activation function\n",
    "def activation(activ_name):\n",
    "    if activ_name=='relu':\n",
    "        return to.nn.ReLU()\n",
    "    elif activ_name=='lrelu':\n",
    "        return to.nn.LeakyReLU()\n",
    "    elif activ_name=='prelu':\n",
    "        return to.nn.PReLU()\n",
    "    elif activ_name=='relu6':\n",
    "        return to.nn.ReLU6()\n",
    "    elif activ_name=='sigmoid':\n",
    "        return to.nn.Sigmoid()\n",
    "    elif activ_name=='tanh':\n",
    "        return to.nn.Tanh()\n",
    "    elif activ_name=='silu':\n",
    "        return to.nn.SiLU()\n",
    "    elif activ_name=='selu':\n",
    "        return to.nn.SELU()\n",
    "    elif activ_name=='celu':\n",
    "        return to.nn.CELU()\n",
    "    elif activ_name=='gelu':\n",
    "        return to.nn.GELU()\n",
    "    else:\n",
    "        return to.nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dAE_Network(to.nn.Module):\n",
    "    def __init__(self, num_X, num_Y, num_Z, henc_nodes, hdec_nodes, hdis_nodes, hactiv_type):\n",
    "        super(dAE_Network, self).__init__()\n",
    "\n",
    "        self.encoder = []\n",
    "        self.encoder.append(to.nn.Linear(num_X, henc_nodes[0]))\n",
    "        self.encoder.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(henc_nodes)-1):\n",
    "            self.encoder.append(to.nn.Linear(henc_nodes[i], henc_nodes[i+1]))\n",
    "            self.encoder.append(activation(hactiv_type))\n",
    "\n",
    "        self.encoder.append(to.nn.Linear(henc_nodes[-1], num_Z))\n",
    "\n",
    "        self.encoder = to.nn.Sequential(*self.encoder).to(device)\n",
    "        for i in self.encoder[::2]:\n",
    "            to.nn.init.kaiming_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "\n",
    "        self.decoder = []\n",
    "        self.decoder.append(to.nn.Linear(num_Z, hdec_nodes[0]))\n",
    "        self.decoder.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(hdec_nodes)-1):\n",
    "            self.decoder.append(to.nn.Linear(hdec_nodes[i], hdec_nodes[i+1]))\n",
    "            self.decoder.append(activation(hactiv_type))\n",
    "\n",
    "        self.decoder.append(to.nn.Linear(hdec_nodes[-1], num_X))\n",
    "\n",
    "        self.decoder = to.nn.Sequential(*self.decoder).to(device)\n",
    "        for i in self.decoder[::2]:\n",
    "            to.nn.init.kaiming_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "\n",
    "        self.disentangler = []\n",
    "        self.disentangler.append(to.nn.Linear(num_Z, hdis_nodes[0]))\n",
    "        self.disentangler.append(activation(hactiv_type))\n",
    "\n",
    "        for i in range(len(hdis_nodes)-1):\n",
    "            self.disentangler.append(to.nn.Linear(hdis_nodes[i], hdis_nodes[i+1]))\n",
    "            self.disentangler.append(activation(hactiv_type))\n",
    "\n",
    "        self.disentangler.append(to.nn.Linear(hdis_nodes[-1], num_Y))\n",
    "\n",
    "        self.disentangler = to.nn.Sequential(*self.disentangler).to(device)\n",
    "        for i in self.disentangler[::2]:\n",
    "            to.nn.init.kaiming_uniform_(i.weight)\n",
    "            to.nn.init.zeros_(i.bias)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = self.encoder(X)\n",
    "        Xr = self.decoder(Z)\n",
    "        Y = self.disentangler(Z)\n",
    "        return Xr, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network,\n",
    "    train_dataloader, beta,\n",
    "    loss_function, optimizer,\n",
    "    tb_writer, epoch_ind\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "    MAPE_list = []\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        X, Y = data\n",
    "\n",
    "        if epoch_ind==0 and i==0:\n",
    "            tb_writer.add_graph(network, X, verbose=False)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictX, predictY = network(X)\n",
    "\n",
    "        loss = beta*loss_function(predictY, Y) + loss_function(predictX, X)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        MAPE = to.mean(to.abs((Y - predictY) / Y)*100)\n",
    "        MAPE_list.append(MAPE.item())\n",
    "    \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "    mean_MAPE = to.mean(to.tensor(MAPE_list, device=device)).item()\n",
    "\n",
    "    return mean_loss, mean_MAPE\n",
    "\n",
    "\n",
    "def valid_epoch(\n",
    "    network,\n",
    "    valid_dataloader, beta,\n",
    "    loss_function\n",
    "    ):\n",
    "\n",
    "    loss_list = []\n",
    "    MAPE_list = []\n",
    "\n",
    "    for i, data in enumerate(valid_dataloader):\n",
    "        X, Y = data\n",
    "        predictX, predictY = network(X)\n",
    "\n",
    "        loss = beta*loss_function(predictY, Y) + loss_function(predictX, X)\n",
    "        loss_list.append(loss.item())\n",
    " \n",
    "        MAPE = to.mean(to.abs((Y - predictY) / Y)*100)\n",
    "        MAPE_list.append(MAPE.item())\n",
    "    \n",
    "    mean_loss = to.mean(to.tensor(loss_list, device=device)).item()\n",
    "    mean_MAPE = to.mean(to.tensor(MAPE_list, device=device)).item()\n",
    "    return mean_loss, mean_MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dAE(\n",
    "    network, beta,\n",
    "    train_dataloader, valid_dataloader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate\n",
    "    ):\n",
    "\n",
    "    if optimizer_type=='adam':\n",
    "        optimizer = to.optim.Adam(network.parameters(), lr=learn_rate)\n",
    "    else:\n",
    "        optimizer = to.optim.SGD(network.parameters(), lr=learn_rate)\n",
    "    \n",
    "    tb_writer = sumwriter('Current_ML_Results/Tensorboard')\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        network.train(True)\n",
    "        mloss, mMAPE = train_epoch(network, train_dataloader,beta, loss_function, optimizer, tb_writer, i)\n",
    "\n",
    "        network.eval()\n",
    "        with to.no_grad():\n",
    "            vmloss, vmMAPE = valid_epoch(network, valid_dataloader,beta, loss_function)\n",
    "        \n",
    "\n",
    "        print('-'*50)\n",
    "        print('Epoch {} / {}'.format(i+1,epochs))\n",
    "        print('-'*15)\n",
    "        print('Average Train Loss : {}'.format(mloss))\n",
    "        print('Average Validation Loss : {}'.format(vmloss))\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch Mean Loss\",\n",
    "                            {\n",
    "                                'Train' : mloss,\n",
    "                                'Validation' : vmloss\n",
    "                            }, i+1)\n",
    "\n",
    "        tb_writer.add_scalars(\"Batch MAPE\",\n",
    "                            {\n",
    "                                'Train' : mMAPE,\n",
    "                                'Validation' : vmMAPE\n",
    "                            }, i+1)\n",
    "\n",
    "    tb_writer.flush()\n",
    "    tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "data = pd.read_csv('Data Workspace/FM_TV_Data.csv')\n",
    "num_freq = 10\n",
    "\n",
    "features = freq_name(num_freq,1,0)\n",
    "labels = ['nu', 'a/b']\n",
    "\n",
    "train_split = int(0.8*len(data))\n",
    "valid_split = len(data)- train_split\n",
    "\n",
    "scaled_data = data[features+labels].copy()\n",
    "scaled_data[freq_name(num_freq,1,0)] = scaled_data[freq_name(num_freq,1,0)]*(100+np.random.normal(3.77,3.18,scaled_data[freq_name(num_freq,1,0)].shape))/100\n",
    "scaled_data[freq_name(num_freq,1,0)] = np.log(scaled_data[freq_name(num_freq,1,0)])\n",
    "# scaled_data['psi'] = np.log(scaled_data['psi'])\n",
    "\n",
    "scaled_data = FFNN_data(scaled_data, features, labels)\n",
    "train_set, valid_set = to_data.random_split(scaled_data, [train_split, valid_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "# Parameters\n",
    "num_X = len(features)\n",
    "num_Y = len(labels)\n",
    "henc_nodes = [10,20,40,80,160,80,40,20,10]\n",
    "hdec_nodes = [10,20,40,80,160,80,40,20,10]\n",
    "hdis_nodes = [10,20,40,80,160,80,40,20,10]\n",
    "hactiv = 'silu'\n",
    "\n",
    "beta = 1\n",
    "num_Z = 10\n",
    "\n",
    "batch_size_train = 200\n",
    "batch_size_valid = 2000\n",
    "\n",
    "epochs = 200\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# Optim Selections\n",
    "loss_function = to.nn.SmoothL1Loss()\n",
    "optimizer_type = 'adam'\n",
    "\n",
    "# Data loaders\n",
    "train_loader = to.utils.data.DataLoader(train_set, batch_size=batch_size_train, shuffle=True)\n",
    "valid_loader = to.utils.data.DataLoader(valid_set, batch_size=batch_size_valid, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = dAE_Network(num_X, num_Y, num_Z, henc_nodes, hdec_nodes, hdis_nodes, hactiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Epoch 1 / 200\n",
      "---------------\n",
      "Average Train Loss : 2.9584429264068604\n",
      "Average Validation Loss : 0.31804585456848145\n",
      "--------------------------------------------------\n",
      "Epoch 2 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.268101304769516\n",
      "Average Validation Loss : 0.2653183341026306\n",
      "--------------------------------------------------\n",
      "Epoch 3 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.2580256164073944\n",
      "Average Validation Loss : 0.26277413964271545\n",
      "--------------------------------------------------\n",
      "Epoch 4 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.2585861384868622\n",
      "Average Validation Loss : 0.2594972848892212\n",
      "--------------------------------------------------\n",
      "Epoch 5 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.25726714730262756\n",
      "Average Validation Loss : 0.2777474522590637\n",
      "--------------------------------------------------\n",
      "Epoch 6 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.2552714943885803\n",
      "Average Validation Loss : 0.2603759169578552\n",
      "--------------------------------------------------\n",
      "Epoch 7 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.2546903192996979\n",
      "Average Validation Loss : 0.25821948051452637\n",
      "--------------------------------------------------\n",
      "Epoch 8 / 200\n",
      "---------------\n",
      "Average Train Loss : 0.254755437374115\n",
      "Average Validation Loss : 0.25805380940437317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\myren\\My Drive\\University - BEng Mechanical\\4th Year\\MRN 412_422\\Revised Code\\ML_dAE.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_dAE(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model, beta,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train_loader, valid_loader,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     loss_function, optimizer_type,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     epochs, learn_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m to\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mCurrent_ML_Results/model.state\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\myren\\My Drive\\University - BEng Mechanical\\4th Year\\MRN 412_422\\Revised Code\\ML_dAE.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_dAE\u001b[1;34m(network, beta, train_dataloader, valid_dataloader, loss_function, optimizer_type, epochs, learn_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     network\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     mloss, mMAPE \u001b[39m=\u001b[39m train_epoch(network, train_dataloader,beta, loss_function, optimizer, tb_writer, i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     network\u001b[39m.\u001b[39meval()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mwith\u001b[39;00m to\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[1;32mc:\\Users\\myren\\My Drive\\University - BEng Mechanical\\4th Year\\MRN 412_422\\Revised Code\\ML_dAE.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(network, train_dataloader, beta, loss_function, optimizer, tb_writer, epoch_ind)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m predictX, predictY \u001b[39m=\u001b[39m network(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m beta\u001b[39m*\u001b[39mloss_function(predictY, Y) \u001b[39m+\u001b[39m loss_function(predictX, X)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss_list\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/myren/My%20Drive/University%20-%20BEng%20Mechanical/4th%20Year/MRN%20412_422/Revised%20Code/ML_dAE.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train_dAE(\n",
    "    model, beta,\n",
    "    train_loader, valid_loader,\n",
    "    loss_function, optimizer_type,\n",
    "    epochs, learn_rate)\n",
    "\n",
    "to.save(model.state_dict(), 'Current_ML_Results/model.state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Errors: \n",
      "--------------------\n",
      "tensor([  3.8314, 100.7179], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(to.load('Current_ML_Results/model.state'))\n",
    "\n",
    "trainvalid_data = pd.read_csv('Data Workspace/FM_TV_Data.csv')\n",
    "scaled_trainvalid_data = trainvalid_data[features+labels].copy()\n",
    "\n",
    "# scaled_trainvalid_data['psi'] = np.log(scaled_trainvalid_data['psi'])\n",
    "scaled_trainvalid_data[freq_name(num_freq,1,0)] = np.log(scaled_trainvalid_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scaled_trainvalid_data = FFNN_data(scaled_trainvalid_data, features, labels)\n",
    "test_loader = to.utils.data.DataLoader(scaled_trainvalid_data, batch_size=len(scaled_trainvalid_data), shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with to.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        X, Y = data\n",
    "        predictX, predictY = model.forward(X)\n",
    "            \n",
    "        Y[:,0] = to.exp(Y[:,0])\n",
    "        predictY[:,0] = to.exp(predictY[:,0])\n",
    "        abs_perc_error = to.abs((Y- predictY)/Y)*100\n",
    "        MAPE_per_dim = to.mean(abs_perc_error, 0)\n",
    "\n",
    "        np.savetxt('Current_ML_Results/MAPE_trainvalid.txt', MAPE_per_dim.cpu().numpy())\n",
    "\n",
    "        print('Absolute Percentage Errors: ')\n",
    "        print('-'*20)\n",
    "        print(MAPE_per_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Percentage Errors: \n",
      "--------------------\n",
      "tensor([  3.8353, 101.0960], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(to.load('Current_ML_Results/model.state'))\n",
    "\n",
    "test_data = pd.read_csv('Data Workspace/FM_Te_Data.csv')\n",
    "scaled_test_data = test_data[features+labels].copy()\n",
    "\n",
    "# scaled_test_data['psi'] = np.log(scaled_test_data['psi'])\n",
    "scaled_test_data[freq_name(num_freq,1,0)] = np.log(scaled_test_data[freq_name(num_freq,1,0)])\n",
    "\n",
    "scaled_test_data = FFNN_data(scaled_test_data, features, labels)\n",
    "test_loader = to.utils.data.DataLoader(scaled_test_data, batch_size=len(scaled_test_data), shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with to.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        X, Y = data\n",
    "        predictX, predictY = model.forward(X)\n",
    "            \n",
    "        Y[:,0] = to.exp(Y[:,0])\n",
    "        predictY[:,0] = to.exp(predictY[:,0])\n",
    "        abs_perc_error = to.abs((Y- predictY)/Y)*100\n",
    "        MAPE_per_dim = to.mean(abs_perc_error, 0)\n",
    "\n",
    "        np.savetxt('Current_ML_Results/MAPE_test.txt', MAPE_per_dim.cpu().numpy())\n",
    "\n",
    "        print('Absolute Percentage Errors: ')\n",
    "        print('-'*20)\n",
    "        print(MAPE_per_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2426e189533ac41f5aad4fbc5dc1b573401ba6ae3fed99d7f2363912ea400c87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
